import glob
import itertools
from torch.utils.data import DataLoader
from torch.utils.data import Sampler, Dataset
from collections import OrderedDict
from random import shuffle
import torch
import pickle
import random
import json
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
import torch
import torch.nn as nn
from tqdm.auto import tqdm
from torch import optim
import datetime
from collections import Counter
import collections
from collections import defaultdict
import itertools
import numpy as np
import statistics
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
import logging
import math
import copy
import torch.nn.functional as F
from nltk.tokenize import word_tokenize
import nltk.translate.bleu_score as bleu
from sklearn.model_selection import train_test_split
import pandas as pd
import torchtext
from torch.nn.utils.rnn import pad_sequence
import gensim
from gensim.models import KeyedVectors
import gc
from rouge import rouge
from bleu import compute_bleu
from torch.utils.tensorboard import SummaryWriter
import nltk
from torch.nn.init import xavier_normal_, xavier_uniform_, constant_
from dist import distinct_n_corpus_level
import os
from scipy import integrate
from scipy.optimize import minimize, fminbound, fsolve


def load_pickle(filename):
    # Return: a list of data.
    with open(filename, "rb") as f:
        reviews = pickle.load(f)
    return reviews


def load_json(filename):
    # Return: a list of data.
    with open(filename, "rb") as f:
        reviews = []
        for line in f.readlines():
            dic = json.loads(line)
            reviews.append(dic)
    return reviews


def load_txt(filename):
    # Return: a list of data.
    with open(filename, "r") as f:
        lines = [s.strip() for s in f.readlines()]
    return lines


def load_model(model, filename):
    """
    Return: a model with saved parameters.
       Example:
           >>>  model = nn.Linear(10,3)   # before load the model, you've to create a new one.
           >>>  model = load_model(model, "savedmodel")
    """
    model.load_state_dict(torch.load(filename))
    return model


def load_stopwords(filename):
    """
    Return: a list of words/str.
      Example:
        >>> filename = "/local/stopwords.txt"
        >>> stop_words = load_stopwords(filename)
    """
    stop_words = []
    f = open(filename, "r")
    for line in f:
        line = line.strip('\n').lower()
        stop_words.append(line)
    return stop_words


def load_glove(vocab):
    """
    load pretrained embedding matrix from Glove. The word does not occur in GloVe vocab will be uniformly randomized.
        Example:
            >>> weights = load_glove(vocab.w)
            >>> word_embedder = nn.Embedding.from_pretrained(weights, freeze=True)
    """

    def unk_init(x):
        return x.uniform_(-0.1, 0.1)

    glove_vocab = torchtext.vocab.GloVe(unk_init=unk_init)
    weights = glove_vocab.get_vecs_by_tokens(vocab.get_itos())
    return weights


def load_word2vec(vocab):
    """
    load pretrained word2vec embedding weights. The unknown word would be randomly generated by uniform distribution.
        Example:
            >>> weights = load_word2vec(vocab.w)
            >>> word_embedder = nn.Embedding.from_pretrained(weights, freeze=True)
    """
    #     word2vec = gensim.downloader.load("word2vec-google-news-300")
    word2vec = KeyedVectors.load("word2vec.wordvectors", mmap='r')
    weights = torch.zeros(len(vocab), 300)
    token_list = vocab.get_itos()
    for i, item in enumerate(token_list):
        # a list of tokens in vocab
        try:  # if the token does not exist in the word2vec vocab:
            idx = vectors.get_index(item)  # obtain the idx and then obtain the corresponding vectors in the word2vec
            weights[i] = torch.tensor(word2vec[idx])
        except:
            weights[i] = torch.randn(300)
    return weights


# 2. Write files.
def write_model(model, filename):
    # Save a model (parameters/weights).
    torch.save(model.state_dict(), filename)


def write_csv(df, filename):
    """
    example:
       >>> write_csv(df, "train.csv")
    """
    # Write the dataframe to a .csv file.
    df.to_csv(filename, index=False, encoding='utf_8_sig')


def write_txt(data, filename):
    # Write a list of data (i.e., string) to the file.
    with open(filename, 'w') as f:
        f.write('\n'.join(data))


def write_dict(data, filename):
    # data is an instance of dict. save the data in .pkl type.
    f = open(filename + ".pkl", "wb")
    pickle.dump(data, f)
    f.close()


# 3. Manipulate data
# 3-1. Retrieval
def list_all(pattern):
    """
    Return all files matching patterns.
        Example:
          >>> pattern = "*.txt"
          >>> match_file(pattern)  # equals to CLI command: ls *.txt
    """
    return glob.glob(pattern)


def sample_list(alist, ratio=0.1, seed=42):
    """
    Sampling a list, given a sampling ratio.
      Example:
          >>> a = [1,2,3,4,5,6,7,8,9,10]
          >>> sample_list(a)
    """
    random.seed(seed)
    sample_len = int(len(alist) * ratio)
    return random.sample(alist, sample_len)


# 3-2. Statistics.
def obtain_classes(df):
    """
    Obtain a class to index dic, and a index to class dic, from the dataframe, in which the last column must be target column.
    Return: tuple of dictionaries
       Example:
           >>> df = pd.read_csv("...")
           >>> classes, cls2idx, idx2cls = obtain_classes_from_df(df)
           ##  cls2idx = {"positive":0, negative:1}, idx2cls = {0:"positive", 1:"negative"}, classes = ["negative", "positive"]
    """
    label = df.columns[-1]
    classes = list(set(df[label]))
    classes.sort()  # the order should be preserved.
    cls2idx, idx2cls = {}, {}
    for i in range(len(classes)):
        cls2idx[classes[i]] = i
        idx2cls[i] = classes[i]
    return classes, cls2idx, idx2cls


def compute_pearson(x, y):
    """
    Calculate a Pearson correlation coefficient for testing non-correlation.
    Param: x, y are 1d tensor/array, or list.

       Example:
          >>> a = torch.randn(20)
          >>> b = torch.randn(20)
          >>> value = compute_pearson(a, b)
             # value = [-1, 1]. -1 means strongly negative correlation,
             # while 1 means strongly positive correlation and 0 value shows no correlation
    """
    assert len(x) == len(y), "x, y should have same shape"
    cov = torch.sum((x - x.mean()) * ((y - y.mean()))) / (len(x) - 1)
    pearson_coef = cov / (x.std() * y.std())
    return pearson_coef.item()


# 3-3. Plot.
def plot_data(Y1, Y2=None, Y3=None):
    """
    Plot the X and Y data, up to three different functions.
    Param Y1: 1d array or list
    Param Y2: 1d array or list
    Param Y3: 1d array or list
      Example:
          >>> y = torch.randn(128)
          >>> plot_data(y)
    """
    Y1_label = "train"
    Y2_label = "valid"
    Y3_label = "test"

    def set_axes(axes, xlabel="epoch", ylabel="loss"):
        # Set the axes for matplotlib
        axes.set_xlabel(xlabel)
        axes.set_ylabel(ylabel)
        axes.grid()

    X = [i for i in range(len(Y1))]
    fig, axes = plt.subplots(figsize=(6, 4))
    set_axes(axes)
    axes.plot(X, Y1, label=Y1_label)
    # If more y are given:
    if Y2 is None:
        pass
    else:
        axes.plot(X, Y2, label=Y2_label)
    if Y3 is None:
        pass
    else:
        axes.plot(X, Y3, label=Y3_label)
    axes.legend()


def plot_bar_distribution(alist):
    """
    Plot the classes' bar distribution given a list.
       Example:
          >>> alist = [0,1,0,1,2,3,4,2,3,4,3,3,3,3,2,2,1,0,1,2,0,3,1,4]
          >>> plot_bar_distribution(alist)
    """
    cnt = Counter()
    for item in alist:
        cnt[item] += 1
    keys = list(cnt.keys())
    nums = []
    for i in range(len(keys)):
        nums.append(cnt[keys[i]])
    # plot
    fig, axes = plt.subplots(figsize=(9, 6))
    axes.bar(keys, nums)
    axes.set_xticklabels(keys)
    axes.set_xticks(range(len(keys)))
    axes.set_xticklabels(keys, rotation="vertical", fontsize=8)
    return cnt


def plot_distribution(alist, bw_adjust=0.15):
    """same usage as plot_bar_distribution. This plots continuous distribution."""
    sns.displot(alist, kind="kde", bw_adjust=bw_adjust)


def plot_heatmap(attn_weights, xlabel="source", ylabel="target", xsticks=None, ysticks=None):
    """
    Plot heatmap, specially for attention weight.
    Parameter: attn_weights, 2d matrix in shape:
        Example:
            >>> attn_weights = torch.randn()
            >>> plot_heatmap(attn_weights)
    """
    if torch.is_tensor(attn_weights):
        attn_weights = attn_weights.detach().numpy()
    assert len(attn_weights.shape) == 2, "only support 2d matrix."
    fig, axes = plt.subplots(figsize=(6, 4))
    if xsticks is None and ysticks is None:
        sns.heatmap(attn_weights, cmap="gray", linewidths=0, vmin=0, vmax=1, ax=axes)
    else:
        sns.heatmap(attn_weights, cmap="gray", linewidths=0, vmin=0, vmax=1, ax=axes, xticklabels=xsticks,
                    yticklabels=ysticks)
    axes.set_xlabel(xlabel, fontsize=12)
    axes.set_ylabel(ylabel, fontsize=12)
    axes.set_title(f'attention weights')


# 3-4. Preprocess.

def flatten(l):
    """
    Flatten a nested list.
        Example:
            >>> a = [1,[2,3]]
            >>> list(flatten(a))   # [1,2,3]
    """
    for el in l:
        if isinstance(el, collections.abc.Iterable) and not isinstance(el, (str, bytes)):
            yield from flatten(el)
        else:
            yield el


def tokenize(sentences):
    """
    lowercase and tokenize.
    Param: "str" standing a sentence or list of str standing a list of sentence.
    Return: example: ["this", "is", "a", "sentence"] for str input, [["sentence"],["one"],["sentence", "two"]] for list of str.
        Example:
            >>> sentence = "The cat was found under the bed."
            >>> tokenizer(sentence) # ["the", "cat", "was", "found", "under", "the", "bed", "."]
    """
    tokenizer = get_tokenizer("basic_english")

    if isinstance(sentences, list):
        tokenized = []
        for sentence in sentences:
            tokenized.append(tokenizer(sentence))
        return tokenized
    elif isinstance(sentences, str):
        return tokenizer(sentences)
    else:
        raise Exception('please check the input type.')


class UIvocab():
    """
    Create Vocab Class for user IDs and item IDs.
        Example:
            >>> user_ids = ['A3UT41TWD7N0D5',
                             'A3H82LUT1EC655',
                             'A27TQA0XCZUXZY',
                             'A15TNUM2PBS6F0',
                             'A3CWH6VKCTJAD',
                             'A1Z54EM24Y40LL']
            >>> u_vocab = UIvocab(user_ids)
            >>> len(u_vocab)
    """

    def __init__(self, ID_list, specials=None):
        self.ID2idx = {}
        self.idx2ID = {}
        if specials is not None:
            if isinstance(specials, str):
                self.ID2idx[specials] = 0
                self.idx2ID[0] = specials
            if isinstance(specials, list):
                for i, item in enumerate(specials):
                    self.ID2idx[item] = i
                    self.idx2ID[i] = item
        for ID in ID_list:
            # Allows only for string type.
            if isinstance(ID, str):
                pass
            else:
                ID = str(ID)
            if ID not in self.ID2idx.keys():
                length = len(self.ID2idx)
                self.ID2idx[ID] = length
                self.idx2ID[length] = ID

    def lookup_tokens(self, idx_list):
        IDs = []
        for idx in idx_list:
            IDs.append(self.idx2ID[idx])
        return IDs

    def lookup_token(self, idx):
        return self.idx2ID[idx]

    def __getitem__(self, IDs):
        try:
            return self.ID2idx[IDs]
        except:
            # out of Vocab
            return self.ID2idx["<unk>"]

    def __len__(self):
        return len(self.ID2idx)


def obtain_textvocab(list_of_strings, granularity="word", max_tokens=20000,
                     special_symbols=['<UNK>', '<PAD>', '<BOS>', '<EOS>', '<SEP>']):
    """
    Build a vocab.
    :param:  a list of strings.
    :return: a vocab class.
       Example:
           >>> sens = ["Hi, how are you doing?", "not bad, you?", "pretty good, thanks"]
           >>> vocab = obtain_textvocab(sens)
    """
    tokenizer = get_tokenizer("basic_english")
    token_iter = []
    if granularity == "word":
        for i in range(len(list_of_strings)):
            try:
                tokenized = tokenizer(list_of_strings[i])
            except:
                continue
            token_iter.append(tokenized)
        vocab = build_vocab_from_iterator(iterator=token_iter, specials=special_symbols, max_tokens=max_tokens)
    #         vocab = build_vocab_from_iterator(iterator=token_iter, specials=special_symbols, max_tokens=20000)
    elif granularity == "char":
        vocab = build_vocab_from_iterator(iterator=list_of_strings, specials=special_symbols, max_tokens=max_tokens)
    #         vocab = build_vocab_from_iterator(iterator=list_of_strings, specials=special_symbols, max_tokens=20000)
    else:
        raise Exception('please check the input type.')
    vocab.set_default_index(vocab["<UNK>"])
    return vocab


def BART_shift_right(input_ids):
    pad_token_id = 1
    """Shift input ids one token to the right, and wrap the last non pad token (usually <eos>)."""
    prev_output_tokens = input_ids.clone()
    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)
    prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()
    prev_output_tokens[:, 1:] = input_ids[:, :-1]
    return prev_output_tokens


def T5_shift_right(input_ids):
    decoder_start_token_id = 0
    pad_token_id = 0

    assert decoder_start_token_id is not None, (
        "self.model.config.decoder_start_token_id has to be defined. In T5 it is usually set to the pad_token_id."
        " See T5 docs for more information"
    )
    shifted_input_ids = input_ids.new_zeros(input_ids.shape)
    shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()
    shifted_input_ids[..., 0] = decoder_start_token_id

    assert pad_token_id is not None, "self.model.config.pad_token_id has to be defined."
    # replace possible -100 values in labels by `pad_token_id`
    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)

    return shifted_input_ids


def obtain_splits(df, seed=42):
    """
    Given a df with [userId, itemId, review, rating], we obtain the TRAIN, VAL, TEST based on the original df.
    Returns: train_df, valid_df, test_df
    Copy from: https://github.com/deepeshhada/ReXPlug/blob/master/preprocess.py#L155
        Example:
            >>> df = pd.read_csv("data.csv")
            >>> train_df, val_df, test_df = obtain_splits(df)
    """

    def get_count(d, i):
        ids = set(d[i].tolist())
        return ids

    data = df
    uid_list, iid_list = get_count(data, 'users'), get_count(data, 'items')
    user_num_all = len(uid_list)
    item_num_all = len(iid_list)

    print("===============Start: Raw Data size======================")
    print(f"total examples: {data.shape[0]}")
    print(f"total number of users: {user_num_all}")
    print(f"total number of items: {item_num_all}")
    print("===============End: Raw Data size========================")
    print(f"-" * 60)

    data_train, data_test = train_test_split(data, test_size=0.2, random_state=seed)
    uids_train, iids_train = get_count(data_train, 'users'), get_count(data_train, 'items')
    user_num = len(uids_train)
    item_num = len(iids_train)

    print("===============Start: no-preprocess: Training Data size======================")
    print("total training examples: {}".format(data_train.shape[0]))
    print("total number of users in training data: {}".format(user_num))
    print("total number of items in training data: {}".format(item_num))
    print("===============End: no-preprocess: Training Data size========================")
    print(f"-" * 60)

    user_frequency_dict = data_train['users'].value_counts().to_dict()
    item_frequency_dict = data_train['items'].value_counts().to_dict()

    uid_misses, iid_misses = [], []
    # Process with the user/item which not shown in TRAIN
    if user_num != user_num_all or item_num != item_num_all:
        uid_misses = list(uid_list - uids_train)
        iid_misses = list(iid_list - iids_train)
    uid_index = []
    # find the missed user/item Ids.
    for uid in uid_misses:
        index = data_test.index[data_test['users'] == uid].tolist()
        uid_index.extend(index)
    data_train = pd.concat([data_train, data_test.loc[uid_index]])
    iid_index = []
    for iid in iid_misses:
        index = data_test.index[data_test['items'] == iid].tolist()
        iid_index.extend(index)
    data_train = pd.concat([data_train, data_test.loc[iid_index]])

    all_index = list(set().union(uid_index, iid_index))
    data_test = data_test.drop(all_index)

    user_frequency_dict = data_train['users'].value_counts().to_dict()
    item_frequency_dict = data_train['items'].value_counts().to_dict()

    data_test, data_val = train_test_split(data_test, test_size=0.5, random_state=seed)
    uid_list_train, iid_list_train = get_count(data_train, 'users'), get_count(data_train, 'items')
    user_num = len(uid_list_train)
    item_num = len(iid_list_train)

    print("===============Start: processed: Training Data size======================")
    print("total training examples: {}".format(data_train.shape[0]))
    print("total number of users in training data: {}".format(user_num))
    print("total number of items in training data: {}".format(item_num))
    print("===============End: processed: Training Data size========================")
    print(f"-" * 60)
    print(f"Train size: {len(data_train)} | Val size: {len(data_val)} | Test size: {len(data_test)}")
    print(f"-" * 60)
    return data_train, data_val, data_test


def obtain_uiReviews(df):
    """
    Groups the user and item reviews.
    Example:
        >>> df = pd.read_csv("train.csv")
        >>> uReviews, iReviews = obtain_uiReviews(df)
    """
    uReviews = {}
    iReviews = {}
    for i in tqdm(range(len(df))):
        uid = df.iloc[i]["users"]
        iid = df.iloc[i]["items"]
        review = df.iloc[i]["explanations"]
        if uid not in uReviews.keys():
            uReviews[uid] = []
        if iid not in iReviews.keys():
            iReviews[iid] = []
        uReviews[uid].append(review)
        iReviews[iid].append(review)
    return uReviews, iReviews


def shingle(text: str, k: int):
    """
    split a text into n-grams.
    Example:
        >>> a = "hello world"
        >>> shingle(a, 3)
    """
    shingle_set = []
    for i in range(len(text) - k + 1):
        shingle_set.append(text[i:i + k])
    return set(shingle_set)


class BucketDataset(Dataset):
    def __init__(self, inputs, targets):
        self.inputs = inputs
        self.targets = targets

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, index):
        if self.targets is None:
            return self.inputs[index]
        else:
            return self.inputs[index], self.targets[index]


class BucketBatchSampler(Sampler):
    def __init__(self, batch_size, inputs, targets):
        if targets is not None:
            if len(inputs) != len(targets):
                raise Exception("[BucketBatchSampler] inputs and targets have different sizes")
        # Remember batch size
        self.batch_size = batch_size
        # For each data item (it's index), keep track of combination of input and target lengths
        self.ind_n_len = []
        if targets is None:
            for i in range(0, len(inputs)):
                self.ind_n_len.append((i, (inputs[i].shape[0], 1)))
        else:
            for i in range(0, len(inputs)):
                self.ind_n_len.append((i, (inputs[i].shape[0], targets[i].shape[0])))

        self.batch_list = self._generate_batch_map()
        shuffle(self.batch_list)
        self.num_batches = len(self.batch_list)

    def _generate_batch_map(self):
        # shuffle all of the indices first so they are put into buckets differently
        shuffle(self.ind_n_len)
        # Organize lengths, e.g., batch_map[(5,8)] = [30, 124, 203, ...] <= indices of sequences of length 10
        batch_map = OrderedDict()
        for idx, length in self.ind_n_len:
            if length not in batch_map:
                batch_map[length] = [idx]
            else:
                batch_map[length].append(idx)
        # Use batch_map to split indices into batches of equal size
        # e.g., for batch_size=3, batch_list = [[23,45,47], [49,50,62], [63,65,66], ...]
        batch_list = []
        for length, indices in batch_map.items():
            for group in [indices[i:(i + self.batch_size)] for i in range(0, len(indices), self.batch_size)]:
                batch_list.append(group)
        return batch_list

    def batch_count(self):
        return self.num_batches

    def __len__(self):
        return self.num_batches

    def __iter__(self):
        for i in self.batch_list:
            yield i


def batchify(X, y, batch_size=32, distributed = False):
    """Batchify different seq length data, without using anying padding.
       Example:
           >>>  X = [torch.randn(5) for i in range(100) ]  # a list of seq in tensor.
           >>>  y = [torch.randn(2) for i in range(100) ]  # a list of seq in tensor.
           >>>  train_dataloader = batchify(X, y, batch_size=batch_size)
           >>>  print(train_dataloader.batch_sampler.num_batches)
    """
    # X: a list of tensor 1d;  y: a list of tensor 1d.
    random.seed(42)
    bucket_batch_sampler = BucketBatchSampler(batch_size, X, y)
    data_iter = BucketDataset(X, y)
    if distributed:
    dataloader = DataLoader(data_iter, batch_sampler=bucket_batch_sampler, num_workers=num_workers,
                            pin_memory=True)  # pin_memory and num_workers for more efficient use of CUDA memory.
    else:
        dataloader = DataLoader(data_iter, batch_sampler=bucket_batch_sampler)
    return dataloader


def batchify_padding(X, y, batch_size=32, num_workers=4, PAD=1):
    """Batchify different seq length data, with using padding.
       Example:
           >>>  X = [torch.randn(5) for i in range(100) ]  # a list of seq in tensor.
           >>>  y = [torch.randn(2) for i in range(100) ]  # a list of seq in tensor.
           >>>  train_dataloader = batchify_padding(X, y, batch_size=batch_size)
    """

    def collate_fn(batch):
        src_batch, tgt_batch = list(zip(*batch))
        src_batch = pad_sequence(src_batch, padding_value=PAD).T
        tgt_batch = pad_sequence(tgt_batch, padding_value=PAD).T
        return src_batch, tgt_batch

    data_iter = BucketDataset(X, y)
    dataloader = DataLoader(data_iter, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers,
                            pin_memory=True)
    return dataloader


def distributed_batchify(X, y, batch_size=32, padding=False, world_size=1):
    """Batchify different seq length data, with using padding.
       Example:
           >>>  X = [torch.randn(5) for i in range(1000) ]  # a list of seq in tensor.
           >>>  y = [torch.randn(2) for i in range(1000) ]  # a list of seq in tensor.
           >>>  data_chunks = distributed_batchify(X, y, batch_size=batch_size)
    """
    if batch_size % world_size != 0:
        raise Exception('batch size must be divisble by world size')
    mini_batch_size = int(batch_size / world_size)
    print(mini_batch_size)
    data_chunks = {}
    total_samples = len(X)
    for i in range(world_size):
        X_chunk = X[i * int((1 / world_size) * total_samples):(i + 1) * int(
            (1 / world_size) * total_samples)]  # slice data accordings to number of device
        y_chunk = y[i * int((1 / world_size) * total_samples):(i + 1) * int((1 / world_size) * total_samples)]
        if padding:
            dataloader = batchify_padding(X_chunk, y_chunk, batch_size=mini_batch_size)
        else:
            dataloader = batchify(X_chunk, y_chunk, batch_size=mini_batch_size)
        print(len(dataloader))
        data_chunks[i] = dataloader
    return data_chunks


# 3-5. Create new data type.
class Logger():
    """
    write the experiment results to log file.
       Example:
           >>> logger = Logger("test.log")
           >>> logger.add_log("hello world")

       Or with arguments:
           >>> config = {"optimizer": "SGD", "lr": 1e-3}
           >>> logger = Logger("test.log", **config)
           >>> logger.add_log(f"epoch {i}, loss: {loss})
    """

    def __init__(self, filename, **kargs):
        msg = "\n-----------------------------ARGUMENTS-----------------------------\n"
        for key in kargs.keys():
            msg += f"{key}                  {kargs[key]}"
            msg += "\n"
        self.filename = filename
        self.write_log(self.filename, msg)

    def write_log(self, filename, msg):
        logging.basicConfig(filename=filename,
                            filemode='a',
                            format='%(asctime)s,%(msecs)d %(name)s: %(message)s',
                            datefmt='%H:%M:%S',
                            level=logging.INFO)
        logging.info(msg)

    def add_log(self, msg):
        self.write_log(self.filename, msg)


class LinearFromScratch(nn.Module):
    """
    Write a Linear layer from scratch.
        Example:
            >>> self.fc = LinearFromScratch(input_size, output_size)   # the same as nn.Linear(input_size, output_size)
    """

    def __init__(self, input_size, output_size):
        super().__init__()
        self.weight = nn.parameter.Parameter(data=torch.ones(output_size, input_size))
        self.bias = nn.parameter.Parameter(data=torch.zeros(output_size))

    def forward(self, X):
        return X @ (self.weight.T) + self.bias


class SequantialFromScratch(nn.Module):
    """
    Write a sequential module from scratch.
        Example:
            >>> self.fc = SequantialFromScratch(
                                    nn.Linear(10,128),
                                    nn.ReLU(),
                                    nn.Linear(128,1)
                                    )
            >>> self.fc(X)
    """

    def __init__(self, *alist):
        super().__init__()
        for i, block in enumerate(alist):
            self.add_module(f"{i}", block)

    def forward(self, X):
        for block in self._modules.values():
            X = block(X)
        return X


class ModuleListFromScratch(nn.Module):
    """
    Write a ModueList module from scratch. Usage: same as a regular Python list object.
        Example:
            >>> self.module = ModuleListFromScratch([
                                    nn.Linear(10,128),
                                    nn.ReLU(),
                                    nn.Linear(128,1)]
                                    )
            >>> self.module[0](X)    # cannot forward directly.
    """

    def __init__(self, alist):
        super().__init__()
        for i, block in enumerate(alist):
            self.add_module(f"{i}", block)

    def __getitem__(self, idx):
        return self._modules(str(idx))


class ModuleDictFromScratch(nn.Module):
    """
    Write a ModueDict module from scratch. Usage: same as a regular Python dictionary object.
        Example:
            >>> self.module = ModuleDictFromScratch({
                                        "linear1":nn.Linear(10,3),
                                        "activation":nn.ReLU(),
                                        "linear2": nn.Linear(3,1)}
                                        )
            >>> self.fc["linear1"](X)    # cannot forward directly.
    """

    def __init__(self, iterable):
        super().__init__()
        try:  # if a list
            for name, key in iterable:
                self.add_module(name, key)
        except:  # a dict
            for name, key in iterable.items():
                self.add_module(name, key)

    def __getitem__(self, key):
        return self._modules[key]


class ParameterListFromScratch(nn.Module):
    """
    Write a ParameterList from scratch.
        Example:
            >>> self.module = ParameterListFromScratch([nn.Parameter(torch.randn(10,10)), nn.Parameter(torch.randn(10,10))])
            >>> self.module[0] * X    #Usage: same as a regular Python list object.
    """

    def __init__(self, alist):
        super().__init__()
        for i, parameter in enumerate(alist):
            self.register_parameter(str(i), parameter)

    def __getitem__(self, idx):
        return list(self.parameters())[idx]


class ParameterDictFromScratch(nn.Module):
    """
    Write a ParameterDict from scratch.
        Example:
            >>> self.module = ParameterDictFromScratch({"linear1": nn.Parameter(torch.randn(10,10)), "linear2": nn.Parameter(torch.randn(10,1))})
            >>> self.module("linear") * X
    """

    def __init__(self, iterable):
        super().__init__()
        try:  # if a list
            for name, key in iterable:
                self.register_parameter(name, key)

        except:  # a dict
            for name, key in iterable.items():
                self.register_parameter(name, key)

    def __getitem__(self, idx):
        for name, parameter in self.named_parameters():
            if idx == name:
                return parameter


class EmbeddingFromScratch(nn.Module):
    """
    Implement the nn.Embedding layer from scratch.
    Equals to nn.Embedding().
        Example:
            >>> X = torch.empty((2,4), dtype=torch.long).random_(10)
            >>> embedding_layer = EmbeddingFromScratch(num_embeddings, embedding_dim, padding_idx=None)
            >>> embedding_layer(X)
    """

    def __init__(self, num_embeddings, embedding_dim, padding_idx=None, _weight=None):
        super().__init__()
        if _weight is None:
            self.weight = nn.Parameter(torch.empty((num_embeddings, embedding_dim)).normal_(mean=0, std=1))
        else:
            self.weight = nn.Parameter(_weight)
        self.padding_idx = padding_idx
        # if padding_idx is specified.
        if self.padding_idx is not None:
            with torch.no_grad():
                self.weight[self.padding_idx] = torch.zeros(embedding_dim)

    def forward(self, X):
        # if X in shape: Seq_len, then return Seq_len x embedding_dim
        # if X in shape : Batch_size x Seq_len, then return Batch_size x Seq_len x embedding_dim
        # if X in shape : ..., then return ... x embedding_dim.
        X_flatten = X.view(-1).clone().detach()
        # get X shape for reshape.
        shape = [X.shape[i] for i in range(len(X.shape))]
        shape.append(self.weight.shape[1])  # embedding_dim
        embeddings = torch.zeros(X_flatten.shape).unsqueeze(1).expand(-1, self.weight.shape[1]).clone()
        for i in range(len(X_flatten)):
            if X_flatten[i] == self.padding_idx:  # do not update padding_idx.
                with torch.no_grad():
                    embeddings[i] = self.weight[X_flatten[i]]
            else:
                embeddings[i] = self.weight[X_flatten[i]]
        return embeddings.view(shape)

    @classmethod
    def from_pretrained(cls, weights, freeze=True, padding_idx=None):
        num_embeddings = weights.shape[0]
        embedding_dim = weights.shape[1]
        embedder = cls(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=padding_idx,
                       _weight=weights)
        embedder.weight.requires_grad = not freeze
        return embedder


class RNNfromScratch(nn.Module):
    """
    Write RNN module from scratch
        Example:
            >>> model = RNNfromScratch(input_size=768, hidden_size=64, batch_first=True)
            >>> X = torch.randn(128,7,768)
            >>> output, hidden = model(X)
            >>> print(output[:,-1,:])
    """

    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        for layer in range(num_layers):
            if layer == 0:
                self.register_parameter("weight_ih_l" + str(layer), nn.Parameter(torch.randn(hidden_size, input_size)))
            else:
                self.register_parameter("weight_ih_l" + str(layer), nn.Parameter(torch.randn(hidden_size, hidden_size)))
            self.register_parameter("weight_hh_l" + str(layer), nn.Parameter(torch.randn(hidden_size, hidden_size)))
            self.register_parameter("bias_ih_l" + str(layer), nn.Parameter(torch.randn(hidden_size)))
            self.register_parameter("bias_hh_l" + str(layer), nn.Parameter(torch.randn(hidden_size)))
        self.act = nn.Tanh()
        self.batch_first = batch_first

    def forward(self, input, hidden=None):
        """
        input size: batch_size x seq_len x input_size (if batch_first=True), otherwise seq_len x batch_size x input_size
        hidden: num_layers x batch_size x hidden_size no matter batch_first true or false.
        """
        assert self.batch_first, "only support batch_first=True"

        batch_size = input.shape[0]
        seq_len = input.shape[1]
        # If hidden not provided, initialized as all zeros.
        # shape: batch_size, num_layers, hidden_size
        if hidden is None:
            hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).permute(1, 0, 2)
        else:
            hidden = hidden.permute(1, 0, 2)
        # batch_size x seq_len x hidden_size
        outputs = torch.zeros(batch_size, seq_len, self.hidden_size)
        ParameterOrderedDict = self.state_dict()
        # Core codes.
        for layer in range(self.num_layers):
            for seq in range(seq_len):
                if layer == 0:
                    i2h = torch.mm(input[:, seq, :], ParameterOrderedDict["weight_ih_l0"].T) + ParameterOrderedDict[
                        "bias_ih_l0"]
                else:
                    i2h = torch.mm(outputs[:, seq, :], ParameterOrderedDict["weight_ih_l" + str(layer)].T) + \
                          ParameterOrderedDict["bias_ih_l" + str(layer)]
                h2h = torch.mm(hidden[:, layer, :], ParameterOrderedDict["weight_hh_l" + str(layer)].T) + \
                      ParameterOrderedDict["bias_hh_l" + str(layer)]
                # h2h2 in shape: batch_size x hidden_size
                output = self.act(i2h + h2h)
                outputs[:, seq, :] = output
                hidden[:, layer, :] = output
        hidden = hidden.permute(1, 0, 2)  # hidden alwasy in shape: 1 x batch_size x hidden_size
        return outputs, hidden


class GRUfromScratch(nn.Module):
    """
    Write nn.GRU module from scratch
        Example:
            >>> model = GRUfromScratch(input_size=768, hidden_size=64, batch_first=True)
            >>> X = torch.randn(1 28,7,768)
            >>> output, hidden = model(X)
            >>> print(output[:,-1,:])
    """

    def __init__(self, input_size, hidden_size, num_layers=1, batch_first=True):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.i2r = []
        self.i2z = []
        self.i2n = []

        self.h2r = []
        self.h2z = []
        self.h2n = []
        for i in range(num_layers):
            if i == 0:
                self.i2r.append(nn.Linear(input_size, hidden_size))
                self.i2z.append(nn.Linear(input_size, hidden_size))
                self.i2n.append(nn.Linear(input_size, hidden_size))

            else:
                self.i2r.append(nn.Linear(hidden_size, hidden_size))
                self.i2z.append(nn.Linear(hidden_size, hidden_size))
                self.i2n.append(nn.Linear(hidden_size, hidden_size))

            self.h2r.append(nn.Linear(hidden_size, hidden_size))
            self.h2z.append(nn.Linear(hidden_size, hidden_size))
            self.h2n.append(nn.Linear(hidden_size, hidden_size))

        self.i2r = nn.ModuleList(self.i2r)
        self.i2z = nn.ModuleList(self.i2z)
        self.i2n = nn.ModuleList(self.i2n)
        self.h2r = nn.ModuleList(self.h2r)
        self.h2z = nn.ModuleList(self.h2z)
        self.h2n = nn.ModuleList(self.h2n)

        self.act = nn.Tanh()
        self.act_other = nn.Sigmoid()

        self.batch_first = batch_first

    def forward(self, input, hidden=None):
        """
        input size: batch_size x seq_len x input_size (if batch_first=True), otherwise seq_len x batch_size x input_size
        hidden: 1 x batch_size x hidden_size no matter batch_first true or false.
        """
        assert self.batch_first, "only support batch_first=True"

        batch_size = input.shape[0]
        seq_len = input.shape[1]
        # If hidden not provided, initialized as all zeros.
        # shape: batch_size, 1, hidden_size
        if hidden is None:
            hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).permute(1, 0, 2)
        else:
            hidden = hidden.permute(1, 0, 2)
        outputs = torch.zeros(batch_size, seq_len, self.hidden_size)
        for layer in range(self.num_layers):
            for seq in range(seq_len):
                if layer == 0:
                    # batch_size x hidden + batch_size x hidden
                    r_t = self.act_other(self.i2r[layer](input[:, seq, :]) + self.h2r[layer](hidden[:, layer, :]))
                    z_t = self.act_other(self.i2z[layer](input[:, seq, :]) + self.h2z[layer](hidden[:, layer, :]))
                    n_t = self.act(
                        self.i2n[layer](input[:, seq, :]) + torch.mul(r_t, self.h2n[layer](hidden[:, layer, :])))
                else:
                    pass
                    r_t = self.act_other(self.i2r[layer](outputs[:, seq, :]) + self.h2r[layer](hidden[:, layer, :]))
                    z_t = self.act_other(self.i2z[layer](outputs[:, seq, :]) + self.h2z[layer](hidden[:, layer, :]))
                    n_t = self.act(
                        self.i2n[layer](outputs[:, seq, :]) + torch.mul(r_t, self.h2n[layer](hidden[:, layer, :])))
                output = torch.mul((1 - z_t), n_t) + torch.mul(z_t, hidden[:, layer, :])
                outputs[:, seq, :] = output
                hidden[:, layer, :] = output
        hidden = hidden.permute(1, 0, 2)  # hidden alwasy in shape: 1 x batch_size x hidden_size
        return outputs, hidden


class LSTMfromScratch(nn.Module):
    """
    Write nn.LSTM module from scratch
        Example:
            >>> model = LSTMfromScratch(input_size=768, hidden_size=64, batch_first=True)
            >>> X = torch.randn(1 28,7,768)
            >>> output, hidden = model(X)
            >>> print(output[:,-1,:])
    """

    def __init__(self, input_size, hidden_size, num_layers=1, batch_first=True):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.i2i = []
        self.i2f = []
        self.i2g = []
        self.i2o = []
        self.h2i = []
        self.h2f = []
        self.h2g = []
        self.h2o = []
        for i in range(num_layers):
            if i == 0:
                self.i2i.append(nn.Linear(input_size, hidden_size))
                self.i2f.append(nn.Linear(input_size, hidden_size))
                self.i2g.append(nn.Linear(input_size, hidden_size))
                self.i2o.append(nn.Linear(input_size, hidden_size))
            else:
                self.i2i.append(nn.Linear(hidden_size, hidden_size))
                self.i2f.append(nn.Linear(hidden_size, hidden_size))
                self.i2g.append(nn.Linear(hidden_size, hidden_size))
                self.i2o.append(nn.Linear(hidden_size, hidden_size))
            self.h2i.append(nn.Linear(hidden_size, hidden_size))
            self.h2f.append(nn.Linear(hidden_size, hidden_size))
            self.h2g.append(nn.Linear(hidden_size, hidden_size))
            self.h2o.append(nn.Linear(hidden_size, hidden_size))
        self.i2i = nn.ModuleList(self.i2i)
        self.i2f = nn.ModuleList(self.i2f)
        self.i2g = nn.ModuleList(self.i2g)
        self.i2o = nn.ModuleList(self.i2o)
        self.h2i = nn.ModuleList(self.h2i)
        self.h2f = nn.ModuleList(self.h2f)
        self.h2g = nn.ModuleList(self.h2g)
        self.h2o = nn.ModuleList(self.h2o)
        self.act_g = nn.Tanh()
        self.act_other = nn.Sigmoid()
        self.batch_first = batch_first

    def forward(self, input, hidden_cell=None):
        """
        input size: batch_size x seq_len x input_size (if batch_first=True), otherwise seq_len x batch_size x input_size
        hidden: 1 x batch_size x hidden_size no matter batch_first true or false.
        """
        assert self.batch_first, "only support batch_first=True"
        batch_size = input.shape[0]
        seq_len = input.shape[1]
        # If hidden not provided, initialized as all zeros.
        # shape: batch_size, 1, hidden_size
        if hidden_cell is None:
            hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).permute(1, 0, 2)
            cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).permute(1, 0, 2)
        else:
            hidden, cell = hidden_cell
            hidden = hidden.permute(1, 0, 2)
            cell = cell.permute(1, 0, 2)  # batch_size x 1 x hidden_size
        outputs = torch.zeros(batch_size, seq_len, self.hidden_size)

        for layer in range(self.num_layers):
            for seq in range(seq_len):
                if layer == 0:
                    i_t = self.act_other(self.i2i[layer](input[:, seq, :]) + self.h2i[layer](
                        hidden[:, layer, :]))  # batch_size x hidden_size
                    f_t = self.act_other(self.i2f[layer](input[:, seq, :]) + self.h2f[layer](hidden[:, layer, :]))
                    g_t = self.act_g(self.i2g[layer](input[:, seq, :]) + self.h2g[layer](hidden[:, layer, :]))
                    o_t = self.act_other(self.i2o[layer](input[:, seq, :]) + self.h2o[layer](hidden[:, layer, :]))
                else:
                    i_t = self.act_other(self.i2i[layer](outputs[:, seq, :]) + self.h2i[layer](
                        hidden[:, layer, :]))  # batch_size x hidden_size
                    f_t = self.act_other(self.i2f[layer](outputs[:, seq, :]) + self.h2f[layer](hidden[:, layer, :]))
                    g_t = self.act_g(self.i2g[layer](outputs[:, seq, :]) + self.h2g[layer](hidden[:, layer, :]))
                    o_t = self.act_other(self.i2o[layer](outputs[:, seq, :]) + self.h2o[layer](hidden[:, layer, :]))
                cell[:, layer, :] = (torch.mul(f_t, cell[:, layer, :]) + torch.mul(i_t, g_t))
                outputs[:, seq, :] = torch.mul(o_t, self.act_g(cell[:, layer, :]))
                hidden[:, layer, :] = outputs[:, seq, :]
        hidden = hidden.permute(1, 0, 2)  # hidden alwasy in shape: 1 x batch_size x hidden_size
        cell = cell.permute(1, 0, 2)  # cell alwasy in shape: 1 x batch_size x hidden_size
        return outputs, (hidden, cell)


class BatchNormFromScratch(nn.Module):
    """
    Perform batch normalization.
    input:batch_size x dimension or, batch_size x dimension x seq_len
    output: same as input.
        Example:
            >>> X = torchr.randn(32, 128, 7)
            >>> layernorm = BatchNormFromScratch((7))
            >>> normed_X = layernorm(X)     # normd_X in shape: 32 x 128 x 7.
    """

    def __init__(self, num_features):
        super().__init__()
        self.w = nn.parameter.Parameter(data=torch.ones(num_features))
        self.b = nn.parameter.Parameter(data=torch.zeros(num_features))

    def forward(self, X):
        # computing on the last axis, which is
        # if X in shape: 32 x 128 x 7, then mean and std in shape: 128.
        # if X in shape: 32 x 128, then mean and std in shape: 128.
        if len(X.shape) == 2:
            axis = 0
            mean = X.mean(dim=axis)  # in shape: num_features
            std = (X.var(dim=axis, unbiased=False) + 1e-5).sqrt()  # in shape: num_features
            normalized = (X - mean) / std  # in shape: batch_size x num_features
        elif len(X.shape) == 3:
            axis = (0, 2)
            mean = X.mean(dim=axis)  # in shape: num_features
            std = (X.var(dim=axis, unbiased=False) + 1e-5).sqrt()  # in shape: num_features
            normalized = ((X.permute(0, 2, 1) - mean) / std).permute(0, 2,
                                                                     1)  # in shape: batch_size x num_featuers x seq_len
        else:
            print("Wrong!")
        output = normalized * self.w.unsqueeze(1) + self.b.unsqueeze(1)
        return output


class LayerNormFromScratch(nn.Module):
    """
    input: Batch_size x Seq_len x dimension
    output: same shape as input.
    Perform Layer normalization.
        Example:
            >>> X = torchr.randn(32, 7, 128)
            >>> layernorm = LayerNormFromScratch((7,128))
            >>> normed_X = layernorm(X)     # normd_X in shape: 32 x 7 x 128.
    """

    def __init__(self, norm_shape):
        super().__init__()
        self.norm_shape = norm_shape
        self.w = nn.parameter.Parameter(data=torch.ones(norm_shape))
        self.b = nn.parameter.Parameter(data=torch.zeros(norm_shape))

    def forward(self, X):
        # computing on the last axis, which is
        # if X in shape: 32 x 7 x 100 and norm_shape in 7x100, then mean and std in shape: 32 x 1 x 1
        # if X in shape: 32 x 7 x 100 and norm_shape in 100, then mean and std in shape: 32 x 7 x 1
        # if X in shape: 32 x 100 and norm in shape 100, then mean and std in shape: 32 x 1
        try:
            axis = [(-i - 1) for i in range(len(self.norm_shape))]  # if norm_shape is a tuple.
            mean = X.mean(dim=axis)[(...,) + (None,) * len(axis)]
            std = (X.var(dim=axis, unbiased=False) + 1e-5).sqrt()[(...,) + (None,) * len(axis)]
        except:
            axis = -1  # if norm_shape is int.
            mean = X.mean(dim=axis)[(...,) + (None,) * 1]
            std = (X.var(dim=axis, unbiased=False) + 1e-5).sqrt()[(...,) + (None,) * 1]

        normalized = (X - mean) / std  # in shape: same as X
        output = normalized * self.w + self.b
        return output


class DropoutFromScratch(nn.Module):
    """
    Implement the dropout regularization from scratch. Usage: the same as nn.Dropout
        Example:
            >>>  dropout = DropoutFromScratch(p=0.1)
            >>>  dropout(X)
    """

    def __init__(self, p):
        super().__init__()
        self.p = p

    def forward(self, X):
        if self.training:
            return torch.zeros_like(X).bernoulli_(1 - self.p) * X * 1 / (1 - self.p)
        else:
            return X


class MultiHeadAttentionFromScratch(nn.Module):
    """
    Write the multi-head attention module from scratch.
    Allows the model to jointly attend to information from different representation subspaces.
    input: query, key, value in shape: Batch_size x Seq_len x Embed_dim
    output: attn_output in shape: Batch_size x Seq_len (target) x Embe_dim,
            attn_weights in shape: Batch_size x Seq_len (target) x Seq_len (source)
       Example:
          >>> tgt = torch.randn(2,6,4)
          >>> src = torch.randn(2,2,4)
          >>> model = MultiHeadAttentionFromScratch()
          >>> outputs, weights = model(tgt,src,src)
          # outputs in shape same as tgt;
          # weights in shape: batch_size x tgt_seq_len x src_seq_len
    """

    def __init__(self, embed_dim=32, num_heads=2, dropout=0, bias=True, batch_first=True):
        # embed_dim must be divisible by num_heads
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.bias = bias
        self.in_proj_weight = nn.Parameter(torch.randn(self.embed_dim * 3, self.embed_dim))
        self.in_proj_bias = nn.Parameter(torch.randn(self.embed_dim * 3))
        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)
        self.head_dim = embed_dim // num_heads
        self.dropout = nn.Dropout(p=dropout)
        assert self.head_dim * num_heads == self.embed_dim, "embed_dim must be divisible by num_heads"
        assert batch_first, "only support batch_first=True"

    def forward(self, query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None):
        """
            param:  query in shape: Batch_size x Seq_len (target) x Embed_Dim
            key in shape:           Batch_size x Seq_len (source) x Embed_Dim
            value in shape:         Batch_size x Seq_len (source) x Embed_Dim
            key_padding_mask in shape: Batch_size x Seq_len (source)
            attn_mask in shape: Seq_len (target) x Seq_len (source), or
                          Batch_size * num_heads x Seq_len (target) x Seq_len (source)
        Note: if source = target, then it is "self-attention"
              else, it is refered to as "multi-head attention" or "cross attention" for clear.
              So, there are three cases:
                        1) all source;
                        2) source and target.
                        3) all target.
        """
        batch_size = query.shape[0]
        src_len = key.shape[1]
        tgt_len = query.shape[1]
        # keep all head attn_weights, and all head attn outputs.
        attn_weights_all = torch.zeros(batch_size, tgt_len, src_len, self.num_heads)
        outputs = []
        q_weight, k_weight, v_weight = self.in_proj_weight.chunk(3)
        q_bias, k_bias, v_bias = self.in_proj_bias.chunk(3)
        for i in range(self.num_heads):
            q_weight_head = q_weight.chunk(self.num_heads)[i]
            k_weight_head = k_weight.chunk(self.num_heads)[i]
            v_weight_head = v_weight.chunk(self.num_heads)[i]

            q_bias_head = q_bias.chunk(self.num_heads)[i]
            k_bias_head = k_bias.chunk(self.num_heads)[i]
            v_bias_head = v_bias.chunk(self.num_heads)[i]

            Q = torch.matmul(query, q_weight_head.T) + q_bias_head
            K = torch.matmul(key, k_weight_head.T) + k_bias_head
            V = torch.matmul(value, v_weight_head.T) + v_bias_head

            scaled = torch.matmul(Q, K.permute(0, 2, 1)) / torch.sqrt(torch.tensor([self.head_dim]))
            # scaled in shape: batch_size x tar_seq_len x src_seq_len
            if attn_mask is not None:
                # attn_mask in shape: tar_seq_len x src_seq_len. (Boolean Type)
                scaled = scaled.masked_fill(attn_mask, -1e30)

            elif key_padding_mask is not None:
                # key_padding_ask in shape: batch_size x src_seq_len
                scaled = scaled.masked_fill(key_padding_mask.unsqueeze(1), -1e30)
            else:
                pass
            # attn_weights in shape: batch_size x tgt_len x src_len
            attn_weights = nn.Softmax(dim=2)(scaled)
            attn_weights = self.dropout(attn_weights)
            output = torch.matmul(attn_weights, V)  # in shape: batch_size x tgt_seq_len x head_dim
            outputs.append(output)
            attn_weights_all[:, :, :, i] = attn_weights

        # average all head weights.
        attn_weights = torch.mean(attn_weights_all, dim=3)
        # concat all head attention output.
        outputs = torch.cat(outputs, dim=2)
        # apply a simple linear transformation
        outputs = self.out_proj(outputs)  # in shape: batch_size x tgt_seq_len x embed_size
        if need_weights:
            return outputs, attn_weights  # weights in shape:
        else:
            return outputs


class TransformerEncoderLayerFromScratch(nn.Module):
    """
    Write Transformer Encoder layer from scratch.
        Example:
            >>> encoder = TransformerEncoderLayerFromScratch(d_model=512, nhead=8, batch_first=True)
            >>> src = torch.randn(32,10,512)
            >>> output = encoder(src)    # output in shape: 32 x 10 x 512 (same as input)
    """

    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout_p=0, batch_first=True):
        super().__init__()
        self.dim_feedforward = dim_feedforward
        self.batch_first = batch_first
        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=batch_first)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(p=dropout_p)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(p=dropout_p)
        self.dropout2 = nn.Dropout(p=dropout_p)
        # norm is after attention and feedforward.

    def forward(self, src, src_mask=None, src_key_padding_mask=None):
        """
        Param: src: batch_size x seq_len x embed_dim
               src_mask: a squared mask
               src_key_padding_mask: batch_size x seq_len     (same as Multi-head Attention)
        """
        assert self.batch_first, "only support batch_first=True"
        # Sublayer 1: attn -> dropout -> residual add -> norm
        attn_output, attn_weights = self.self_attn(src, src, src, attn_mask=src_mask,
                                                   key_padding_mask=src_key_padding_mask)
        attn_output = self.dropout1(attn_output)
        sublayer1 = self.norm1(attn_output + src)  # add and norm

        # Sublayer 2: feedforward -> residual add -> norm
        ffn_output = self.linear2(self.dropout(nn.ReLU()(self.linear1(sublayer1))))
        ffn_output = self.dropout2(ffn_output)
        sublayer2 = self.norm2(ffn_output + sublayer1)  # add and norm
        return sublayer2


class TransformerEncoderFromScratch(nn.Module):
    """
    Write Transformer Encoder from scratch.
    param: Transformer encoder_layer
           num_layers
        Example: >>> encoder_layer = TransformerEncoderLayerFromScratch(d_model=512, nhead=8, batch_first=True)
                 >>> encoder = TransformerEncoderFromScratch(encoder_layer, num_layers=2)
                 >>> src = torch.randn(32,10,512)
                 >>> output = encoder(src)    # output in shape: 32 x 10 x 512 (same as input)
    """

    def __init__(self, encoder_layer, num_layers, norm=None):
        super().__init__()
        self.layers = []
        self.num_layers = num_layers
        for i in range(num_layers):
            self.layers.append(encoder_layer)
        self.layers = nn.ModuleList(self.layers)
        self.norm = norm

    def forward(self, src, mask=None, src_key_padding_mask=None):
        output = src
        for layer in self.layers:
            output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)
        if self.norm is not None:
            output = self.norm(output)
        return output


class TransformerDecoderLayerFromScratch(nn.Module):
    """
    Write Transformer Decoder Layer from scratch.
            Example:
                 >>> decoder_layer = TransformerDecoderLayerFromScratch(d_model=512, nhead=8, batch_first=True)
                 >>> tgt = torch.randn(32,7,512)
                 >>> src = torch.randn(32,10,512)
                 >>> output = decoder_layer(tgt, src)    # output in shape: 32 x 7 x 512 (same as tgt)
    """

    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout_p=0, batch_first=True):
        super().__init__()
        self.dim_feedforward = dim_feedforward
        self.batch_first = batch_first
        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=batch_first)
        self.multihead_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=batch_first)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(p=dropout_p)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(p=dropout_p)
        self.dropout2 = nn.Dropout(p=dropout_p)
        self.dropout3 = nn.Dropout(p=dropout_p)

    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None,
                memory_key_padding_mask=None):
        assert self.batch_first, "only support batch_first=True"
        # Sublayer 1: attn -> dropout -> residual add -> norm
        attn_output, attn_weights = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,
                                                   key_padding_mask=tgt_key_padding_mask)
        attn_outout = self.dropout1(attn_output)
        sublayer1 = self.norm1(attn_output + tgt)  # add and norm

        # Sublayer 2: multi-attn -> dropout -> residual add -> norm
        multi_attn_output, multi_attn_weights = self.multihead_attn(sublayer1, memory, memory, attn_mask=memory_mask,
                                                                    key_padding_mask=memory_key_padding_mask)
        multi_attn_output = self.dropout2(multi_attn_output)
        sublayer2 = self.norm2(multi_attn_output + sublayer1)

        # Sublayer 3: feedforward -> residual add -> norm
        ffn_output = self.linear2(self.dropout(nn.ReLU()(self.linear1(sublayer2))))
        ffn_output = self.dropout3(ffn_output)
        sublayer3 = self.norm3(ffn_output + sublayer2)  # add and norm

        return sublayer3


class TransformerDecoderFromScratch(nn.Module):
    """
    Write Transformer Decoder from scratch.
    param: Transformer encoder_layer
           num_layers
        Example: >>> decoder_layer = TransformerDecoderLayerFromScratch(d_model=512, nhead=8, batch_first=True)
                 >>> decoder = TransformerDecoderFromScratch(decoder_layer, num_layers=2)
                 >>> tgt = torch.randn(32, 7, 512)
                 >>> src = torch.randn(32,10,512)
                 >>> output = decoder(tgt, src)    # output in shape: 32 x 7 x 512 (same as tgt)
    """

    def _get_clones(module, N):
        import copy
        return ModuleList([copy.deepcopy(module) for i in range(N)])

    def __init__(self, decoder_layer, num_layers, norm=None):
        super().__init__()
        self.layers = []
        self.num_layers = num_layers
        self.layers = _get_clones(encoder_layer, num_layers)
        self.norm = norm

    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None,
                memory_key_padding_mask=None):
        output = tgt
        for layer in self.layers:
            output = layer(output, memory,
                           tgt_mask=tgt_mask, memory_mask=memory_mask,
                           tgt_key_padding_mask=tgt_key_padding_mask,
                           memory_key_padding_mask=memory_key_padding_mask)
        if self.norm is not None:
            output = self.norm(output)
        return output


class TransformerFromScratch(nn.Module):
    """
    Write the Transformer model.
    Reference: Vaswani, Ashish, et al. "Attention is all you need." NIPS '17.
               https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
      Example:
          >>> transformer_model = TransformerFromScratch(nhead=16, num_encoder_layers=12, batch_first=True)
          >>> src = torch.rand((2, 7, 512))
          >>> tgt = torch.rand((2, 10, 512))
          >>> output = transformer_model(src, tgt)
          # output in shape 2 x 10 x 512, the same as tgt.
    """

    def __init__(self, d_model=512, nhead=8,
                 num_encoder_layers=6,
                 num_decoder_layers=6,
                 dim_feedforward=2048,
                 dropout=0.1,
                 batch_first=True):
        super().__init__()
        norm = nn.LayerNorm(d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,
                                                   dropout=dropout, batch_first=batch_first)
        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,
                                                   dropout=dropout, batch_first=batch_first)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers, norm=norm)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers, norm=norm)

    def forward(self, src, tgt,
                src_mask=None, tgt_mask=None,
                memory_mask=None,
                src_key_padding_mask=None,
                tgt_key_padding_mask=None,
                memory_key_padding_mask=None):
        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)
        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,
                              tgt_key_padding_mask=tgt_key_padding_mask,
                              memory_key_padding_mask=memory_key_padding_mask)
        return output


# 2. Useful functions & models.
def show_parameters(model):
    for name, parameter in model.named_parameters():
        print(f"{name}, shape: {parameter.shape} \n  {parameter}")
        print("\n" + "---" * 20 + "\n")


def get_clones(module, N):
    """
    Used for clone modues.
        Example:
            >>> fc = nn.Linear(3,3)
            >>> module = get_clones(fc, 10)   # equals "for" loop.
    """
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])


def freeze_model(model):
    """
    Given a PyTorch model or module, we freeze all the paramters.
    Note that this freeze operation orthogonal to where the model is in train mode or eval mode. That's to say,
    we only fix all the trainble parameters of the model so that it cannot be updated. Feel free to use it either
    in training process or evaluation process.
        Example:
            >>> model = nn.Sequential(nn.Linear(10, 128), nn.ReLU(), nn.Linear(128, 1))
            >>> freeze(model)
    Note: you also need to set filter to the optimzier, e.g., filter(lambda p: p.requires_grad, model.parameters())
    """
    for p in model.parameters():
        p.requires_grad = False


def unfreeze_model(model):
    """
    Opposite to freeze_model.
    """
    for p in model.parameters():
        p.requires_grad = True


class alpha_classifier(nn.Module):
    def __init__(self, vocab_size=20000, embed_dim=300, hidden_dim=1024, output_size=2, device="cuda"):
        super().__init__()
        self.embedder = nn.Embedding(vocab_size, embed_dim)
        self.output = nn.Linear(embed_dim, output_size)
        self.init_weights()
        self = self.to(device)
        self.device = device
        self.loss_fn = nn.CrossEntropyLoss()

    def init_weights(self):
        initrange = 0.1
        self.embedder.weight.data.uniform_(-initrange, initrange)

    def forward(self, X):
        # obtain the representation of the sentence.
        # X in shape (N, seqlen)
        input = self.embedder(X).mean(dim=1)  # (N, embed_dim)
        output = self.output(input)
        return output

    def trainIter(self, dataloader, optimizer, use_amp):
        # switch to train mode
        self.train()
        scaler = torch.cuda.amp.GradScaler()
        for X, label in dataloader:
            optimizer.zero_grad()
            X = X.to(self.device)
            label = label.to(self.device).reshape(-1)
            if use_amp:
                with torch.cuda.amp.autocast():
                    pred = self.forward(X)
                    loss = self.loss_fn(pred, label)
                scaler.scale(loss).backward()
                nn.utils.clip_grad_norm_(self.parameters(), 1)
                scaler.step(optimizer)
                scaler.update()
            else:
                pred = self.forward(X)  # in shape (N, 2)
                # compute loss
                loss = self.loss_fn(pred, label)
                loss.backward()
                nn.utils.clip_grad_norm_(self.parameters(), 1)  # If any parameter's gram norm > 1, then clip.
                optimizer.step()

    def evaluate(self, dataloader, metric):
        # switch to eval mode
        self.eval()
        metric_dic = defaultdict(float)
        count = 0
        with torch.no_grad():
            for X, label in dataloader:
                X = X.to(self.device)
                y = label.to(self.device).reshape(-1)
                pred = self.forward(X)

                for item in metric:
                    if item == "loss":
                        loss = self.loss_fn(pred, y)
                        metric_dic[item] += float(loss)  # cuda to cpu device.
                    if item == "precision":
                        pred_digit = pred.topk(1).indices.reshape(-1)
                        precision = (pred_digit == y).sum() / X.shape[0]  # devided by batch size
                        metric_dic[item] += float(precision)  # cuda to cpu device
                count += 1
        for item in metric_dic.keys():
            metric_dic[item] = metric_dic[item] / count  # avg. = sum/count
        return metric_dic

class MLP(nn.Module):
    '''
    Multilayer Perceptron
    Inputs:
    input_var: shape=(batch_size, dim)
    Outputs:
    output_var: shape=(batch_size, layer_sizes[-1])
     Example:
           >>> dim, hidden_dims = 1024, [512, 512, 1]
           >>> mlp = MLP(dim, hidden_dims)
           >>> x = torch.randn(32,1024)     # x in shape (batch_size, dim)
           >>> output = mlp(x)
    '''

    def __init__(self, dim, layer_sizes, act='relu'):
        super().__init__()
        act_fn = dict(relu=nn.ReLU,
                      sigmoid=nn.Sigmoid,
                      tanh=nn.Tanh,
                      leakyrelu=nn.LeakyReLU)[act]
        layers = []
        i_size = dim
        for o_size in layer_sizes:
            layers.append(nn.Linear(i_size, o_size))
            layers.append(act_fn())
            i_size = o_size
        self.layers = nn.Sequential(*layers)

    def forward(self, input_var):
        return self.layers(input_var)


class toy_regression(nn.Module):
    """
    Define a toy regression model
        Example:
            >>> X = torch.randn(32, 768)      # batch_size x feature size
            >>> model = toy_classifier(768)
            >>> pred = model(X)               # pred in shape: 32 x 1
    """

    def __init__(self, input_size):
        super(toy_regression, self).__init__()
        self.predict = nn.Sequential(
            nn.Linear(input_size, 2048),
            nn.ReLU(),
            nn.Linear(2048, 256),
            nn.ReLU(),
            nn.Linear(256, 2048),
            nn.ReLU(),
            nn.Linear(2048, 1),
        )

    def forward(self, X):
        pred = self.predict(X)
        return pred


class toy_classifier(nn.Module):
    """
    Define a toy classifier model.
        Example:
            >>> X = torch.randn(32, 768)    # batch_size x feature size
            >>> model = toy_classifier(768, 10)   # 10 classes
            >>> pred = model(X)        # pred in shape: 32 x 10
    """

    def __init__(self, input_size, output_size):
        super(toy_classifier, self).__init__()
        self.predict = nn.Sequential(
            nn.Linear(input_size, 2048),
            nn.ReLU(),
            nn.Linear(2048, 256),
            nn.ReLU(),
            nn.Linear(256, 2048),
            nn.ReLU(),
            nn.Linear(2048, output_size),
            nn.LogSoftmax(),
        )

    def forward(self, X):
        pred = self.predict(X)
        return pred


class LSTM_classifier(nn.Module):
    """
    Define a toy classifier model based on sequential data. e.g., name classifier.
        Example:
            >>>  X = torch.empty((32,7), dtype=torch.long).random_(1000)   # X in shape: batch_size x sequence length
            >>>  model = LSTM_classifier(vocab_size=1000, embed_size=200, hidden_size=200, lstm_layers=1, output_size=10)  # 10 classes
            >>>  pred = model(X)     # pred in shape: 32 x 10.
    """

    def __init__(self, vocab_size, embed_size, hidden_size, lstm_layers, output_size):
        super().__init__()
        self.embedder = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=lstm_layers, batch_first=True)
        self.fw = nn.Sequential(
            nn.Linear(hidden_size, 512),
            nn.ReLU(),
            nn.Linear(512, output_size),
            nn.LogSoftmax(dim=1),
        )

    def init_weights(self):
        initrange = 0.5
        self.embedder.weight.data.uniform_(-initrange, initrange)

    def forward(self, X):
        input = self.embedder(X)  # input in shape: batch_size x seq_len x embed_size
        output = self.lstm(input)[0][:, -1, :]
        output = self.fw(output)
        return output


class Seq2Seq_model(nn.Module):
    """
    Create a toy Seq2Seq model based on Transformer.
        Example:
            >>> src = torch.zeros((32,7), dtype=torch.long).random_(1000) # batch_size x src_seq_len
            >>> tgt = torch.zeros((32,6), dtype=torch.long).random_(1000) # batch_size x tgt_seq_len
            >>> tgt_mask = generate_square_mask_fromscratch(6)
            >>> model = Seq2Seq_model(vocab_size=1000, embed_size=256)
            >>> output = model(src, tgt, tgt_mask)         # batch_size x tgt_seq_len
    """

    def __init__(self, vocab_size, embed_size):
        super().__init__()
        self.embed_size = embed_size
        self.src_embedder = nn.Embedding(vocab_size, embed_size)
        self.tgt_embedder = nn.Embedding(vocab_size, embed_size)
        self.positioner = PositionalEncoding(embed_size=embed_size)
        self.transformer = nn.Transformer(d_model=embed_size, batch_first=True)
        self.fc = nn.Linear(embed_size, vocab_size)
        self.logsoftmax = nn.LogSoftmax(dim=2)

    def forward(self, src_token, tgt_token, tgt_mask):
        # encode
        memory = self.encode(src_token)
        # decode
        output = self.decode(tgt_token, memory, tgt_mask)
        return output

    def encode(self, src_token):
        src = self.src_embedder(src_token) * math.sqrt(self.embed_size)
        src = self.positioner(src)
        return self.transformer.encoder(src)

    def decode(self, tgt_token, memory, tgt_mask):
        tgt = self.tgt_embedder(tgt_token) * math.sqrt(self.embed_size)
        tgt = self.positioner(tgt)
        output = self.transformer.decoder(tgt, memory, tgt_mask=tgt_mask)
        return self.logsoftmax(self.fc(output))


class PositionalEncoding(nn.Module):
    """
    Positional Encoding. Warning: DO NOT FORGET SET TO "eval()" when doing inference.
        Example:
            >>> X = torch.randn(32,7,200)
            >>> positioner = PositionalEncoding(emb_size=200, dropout=0.1)
            >>> positioner(X)    # the same shape as X.
    """

    def __init__(self, embed_size, dropout=0.1, maxlen=5000):
        super().__init__()
        den = torch.exp(- torch.arange(0, embed_size, 2) * math.log(10000) / embed_size)
        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
        pos_embedding = torch.zeros((maxlen, embed_size))
        pos_embedding[:, 0::2] = torch.sin(pos * den)
        pos_embedding[:, 1::2] = torch.cos(pos * den)
        pos_embedding = pos_embedding.unsqueeze(-2)
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('pos_embedding', pos_embedding)

    def forward(self, token_embedding):
        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])


def generate_square_mask(seqlen):
    """
    Generate squared mask.
        Example:
            >>> mask = generate_square_mask_fromscratch(3)
                    # mask = tensor[[False, True, True],
                    #               [False, False, True],
                    #               [False, False, False]]
    """
    mask = torch.triu(torch.ones((seqlen, seqlen)), diagonal=1) == 1
    return mask


def generate_mask(row, column):
    """
    Generate  mask.
        Example:
            >>> mask = generate_square_mask_fromscratch(3, 4)
            # mask = tensor[[False, True, True, True],
            #               [False, False, True, True],
            #               [False, False, False, True]]
    """
    if row < column:
        mask = torch.triu(torch.ones((column, column)), diagonal=1) == 1
        mask = mask[:row, :]
    else:
        mask = torch.triu(torch.ones((row, row)), diagonal=1) == 1
        mask = mask[:, :column]
    return mask


def generate_peter_mask(seq_len):
    """
    Generate squared mask.
        Example:
            >>> mask = generate_peter_mask_fromscratch(4)
            # mask = tensor[[False, False, True, True],
            #               [False, False, True, True],
            #               [False, False, False, True],
            #.              [Flase, False, False, False]]
    """
    mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1) == 1
    mask[0, 1] = False
    return mask


def xavier_normal_initialization(module):
    """
    Example:
        >>> self.apply(xavier_normal_initialization)
    """
    if isinstance(module, nn.Embedding):
        xavier_normal_(module.weight.data)
    elif isinstance(module, nn.Linear):
        xavier_normal_(module.weight.data)
        if module.bias is not None:
            constant_(module.bias.data, 0)


def create_mask(source, target):
    """
    create the mask for transformer model.
        Example:
            >>> source =  torch.empty((32,6), dtype=torch.long).random_(5)
            >>> target =  torch.empty((32,7), dtype=torch.long).random_(5)
            >>> src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(source, target)

    """
    PAD_IDX = 1
    src_seq_len = source.shape[1]
    tgt_seq_len = target.shape[1]

    # attention mask
    src_mask = torch.zeros((src_seq_len, src_seq_len), dtype=torch.bool)
    tgt_mask = generate_square_mask(tgt_seq_len)

    # padding mask
    src_padding_mask = (source == PAD_IDX)
    tgt_padding_mask = (target == PAD_IDX)
    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask


def count_param(model):
    from numerize import numerize
    count = 0
    for parameter in model.parameters():
        if parameter.requires_grad:
            count += len(parameter.view(-1))
    return numerize.numerize(count)


def obtain_lr(optimizer):
    groups = len(optimizer.param_groups)
    lrs = []
    for i in range(groups):
        lrs.append(optimizer.param_groups[i]["lr"])
    return lrs


class JoTokenizer():
    """
    Create a unified vocab for user, item and words, which is written in the same style of Tokenizer in Transformers lib.
    It helps to pre-process and post-processs the data, for example, converts the text data to model input format, or converts the IDs back to tokens, etc.
    You can also inherit from this class for further customization.
        Example:
            >>> u_vocab = UIvocab(["A001", "A0032"])
            >>> i_vocab = UIvocab(["B00032", "B0013", "B0032", "B32whatever"])
            >>> w_vocab = obtain_textvocab(["hello world, this is the first sentence", "this is the second one", ...])
            >>> tokenizer = MyTokenizer(u_vocab, i_vocab, w_vocab)
            >>> tokenizer(sample)  # sample is a dict type, or one row of dataframe, or one row of datasets,...
    """

    def __init__(self, u_vocab, v_vocab, w_vocab):
        self.u = u_vocab
        self.v = v_vocab
        self.w = w_vocab
        self.BOS = self.lookup(self.w, "<BOS>")
        self.EOS = self.lookup(self.w, "<EOS>")
        self.PAD = self.lookup(self.w, "<PAD>")

    def __call__(self):
        """
        The basic use is to compile a record to word IDs and user/item IDs.
        param: sample,  is a dict object, which conincides with one row of dataframe (i.e., df.loc)
        You may need to rewrite it for further customization.
            Example:
                >>> sample = df.loc[0]
                >>> tokenizer = MyTokenizer(u_vocab, v_vocab, w_vocab)
                >>> encoded = tokenizer.encode(sample)
                ### output is encoded: {'user_id': tensor([853]),
                               'item_id': tensor([158]),
                               'rating': tensor([4]),
                               'explanation': tensor([    2,     4,  1897, 19424,     6,  9120,    16, 14398,
                                                          4,    60,   616,     6,    77,   605,     3]),
                                'input_ids': tensor([853, 158]),
                                'output_ids': tensor([    4,     2,     4,  1897, 19424,     6,  9120,  16,
                                                        14398,     4,  60,   616,     6,    77,   605,     3])}
        """
        raise Exception("__call__ Method Not Implemented!")

    def decode(self, output_ids, return_sentence=True):
        """
        Decode the text field (in shape (N,seqlen)) to truncated list, i.e., a list of list of tokens, and further to a list of sentences.
        param: output_tensors, a tensor of batched sequence. Note that we only support (N, seqlen) shape, N could be 1.
            Example:
                >>> output_ids = torch.tensor([[31,12,23,4,5,5,3], [423,233,122,3,6,3,19]])
                >>> decoded = tokenizer.truncate(output_ids)
                    # the output is like:
                                {'truncated_seq': [[31, 12, 23, 4, 5, 5], [423, 233, 122]],
                                 'truncated_sen': ['on this for the a a', 'career fans two']}
        Similar to Transformers tokenizer.decode(output_ids) function when "reutnr_sentence" set to True.
        This method is used for post-processing for evaluation of texts (and examine the generated sentences)
        """
        truncated = []
        truncated_sentences = []
        for seq in output_ids:
            truncated_seq = self.truncate_after_eos(seq.tolist(), self.EOS)  # a list of ids, removing ids after EOS.
            truncated.append(truncated_seq)
            if return_sentence:
                sentence = " ".join([self.lookup(self.w, ids) for ids in truncated_seq])
                truncated_sentences.append(sentence)
        if return_sentence:
            return {"truncated_seq": truncated, "sentences": truncated_sentences}
        else:
            return {"truncated_seq": truncated}

    @staticmethod
    def lookup(vocab, lookup_item):
        """
        It looks up tokens/IDs from idx, or looks up idx from tokens/IDs.
        This function combines Transformers Tokenizer .convert_ids_to_tokens and .convert_tokens_to_ids together.
        param: vocab: can be one of the u_vocab, i_vocab, or w_vocab.
        Return: a list or item.
            Example:
                >>> idx = 0
                >>> token = MyTokenizer.lookup(w_vocab, idx)
                # equals to w_vocab.lookup_token(idx)
            Or,
                >>> idx = [0,1,2,3]
                >>> tokens = MyTokenizer.lookup(w_vocab, idx)
                # equals to w_vocab.lookup_tokens(idx)
            Or,
                >>> token = "hello"
                >>> idx = MyTokenizer.lookup(w_vocab, token)
                # equals to w_vocab([token])
            Or,
                >>> tokens = ["hello", "word"]
                >>> idxs = MyTokenizer.lookup(w_vocab, tokens)
                # equals to w_vocab(tokens)
        """
        if isinstance(lookup_item, int):
            idx = lookup_item
            return vocab.lookup_token(idx)

        elif isinstance(lookup_item, str):
            IDs_token = lookup_item
            return vocab[IDs_token]

        elif isinstance(lookup_item, list):
            if isinstance(lookup_item[0], str):
                ID_list = lookup_item  #
                idx_list = [vocab[ID] for ID in ID_list]
                return idx_list
            else:
                idx_list = lookup_item
                ID_list = vocab.lookup_tokens(idx_list)
                return ID_list
        elif torch.is_tensor(lookup_item):
            idx = lookup_item.tolist()
            return vocab.lookup_tokens(idx)
        else:
            raise Exception('please check the input type.')

    @staticmethod
    def truncate_after_eos(alist, EOS=3):
        new_list = []
        for item in alist:
            if item == EOS:
                break
            new_list.append(item)
        return new_list


class JoModule(nn.Module):
    """
    Defining a parent class that simplies PyTorch model codes.
    You only need to re-write methods 'training_step' and 'validation_step', and 'forward'
    """

    def __init__(self):
        super().__init__()

    def training_step(self, batch=None, device=None):
        """ Should return loss value with gradients.
            Example:
                >>> X, y = batch
                >>> X = X.to(device)
                >>> y = y.to(device)
                >>> loss = self.forward(X, y)   return loss.
        """
        raise NotImplementedError("Method 'training_step' not implemented.")

    def validation_step(self, batch=None, device=None, metrics=None):
        """ Could return loss, or metric evaluated on validation dataset, such as precision,
            Example:
                >>> X, y = batch
                >>> X = X.to(device)
                >>> y = y.to(device)
                >>> loss = self.forward(X, y)
                >>> predicion = ...
                >>> return {"loss": loss, "precision": prediction}   return dict type.
        """
        raise NotImplementedError("Method 'training_step' not implemented.")

    def trainIter(self, dataloader=None, device="cpu", optimizer=None, use_amp=True, max_clip_norm=1, accum_step=8):
        # tricks here: 1. use automatic mixed precision.  2. use accumulated gradients.
        ACCUM_GRAD_STEP = accum_step
        if use_amp:
            scaler = torch.cuda.amp.GradScaler()
            if device == "cpu":
                raise Exception("Currently automatic memory precision is not supported for cpu.")
        # set to train mode
        self.train()
        for batch_idx, batch in enumerate(tqdm(dataloader, leave=False)):
            if use_amp:
                with torch.cuda.amp.autocast():
                    loss = self.training_step(batch, device) / ACCUM_GRAD_STEP  # gradients normalization
                    scaler.scale(loss).backward()
                if ((batch_idx + 1) % ACCUM_GRAD_STEP == 0) or (
                        batch_idx + 1 == len(dataloader)):  # forward every 8 iters
                    nn.utils.clip_grad_norm_(self.parameters(), max_clip_norm)
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
            else:
                loss = self.training_step(batch, device) / ACCUM_GRAD_STEP
                loss.backward()
                if ((batch_idx + 1) % ACCUM_GRAD_STEP == 0) or (batch_idx + 1 == len(dataloader)):
                    nn.utils.clip_grad_norm_(self.parameters(), max_clip_norm)
                    optimizer.step()
                    optimizer.zero_grad()
        # after training every epoch, we release the memory.
        del batch
        gc.collect()
        torch.cuda.empty_cache()

    def validIter(self, dataloader=None, device="cpu", metrics=["loss"], efficient_valid=True):
        """
        Metrics should be List type. e.g., metrics = ["loss", "precision", "recall", "bleu", "ndcg"].
        param: "efficient_valid", boolean type, for faster evlauation. For example, you hope to evaluate only a part of validation dataset; or you hope to compute only a part of metrics for preliminary tuning.
        When reporting results on the **test set**, you need to set "efficient_valid" to **False**. That is, reporting all metrics scores on the whole test set.
        """
        # set to eval mode
        self.eval()
        results = defaultdict(float)
        if efficient_valid:
            total_size = int(len(dataloader) / 4)  # evlauate only a quarter of the dataloader for efficience.
        else:
            total_size = len(dataloader)
        with torch.no_grad():
            tqdm_iterator = tqdm(dataloader, leave=False)  # remove tqdm bar when loop is over
            for batch_idx, batch in enumerate(tqdm_iterator):
                evaluated = self.validation_step(batch, device, metrics)
                keys = evaluated.keys()
                for key in keys:
                    results[key] += evaluated[key]
                if batch_idx >= total_size - 1:
                    break
        for key in keys:
            results[key] = results[key] / total_size
        # after evaluation, we release the memory.
        del batch
        gc.collect()
        torch.cuda.empty_cache()
        return results  # return a dic


class GerModule(JoModule):
    """
    YoModule inherits from JoModule, with the rewrite of validIter method and other useful method.
    For child model, You need to rewrite "forward", "training_step", and "validIter" methods.
    """

    def __init__(self):
        super().__init__()
        self.rating_loss = nn.MSELoss()
        self.text_loss = nn.CrossEntropyLoss()

    def recommend(self, input_ids, device):
        raise Exception("Recommendation Module Not Implemented!")

    def generate(self, input_ids, device):
        raise Exception("Generate Method Not Implemented!")

    def gather(self, batch, device):
        raise Exception("Gather Module Not Implemented!")

    def _compute_mse_loss_(self, preds, refs):
        return self.rating_loss(preds, refs)

    def _compute_cross_loss_(self, preds, refs):
        return self.text_loss(preds, refs)

    @staticmethod
    def _evaluate_rating_(preds, refs):
        result = compute_mae_rmse(preds, refs)
        return {"MAE": result[0], "RMSE": result[1]}

    @staticmethod
    def _evaluate_text_(preds, refs):
        rouge_results = rouge_score(preds, refs)
        preds_tokens = [tokenize(sentence) for sentence in preds]
        refs_tokens = [tokenize(sentence) for sentence in refs]
        b1 = bleu_score(preds_tokens, refs_tokens, n_gram=1)
        b2 = bleu_score(preds_tokens, refs_tokens, n_gram=2)
        b3 = bleu_score(preds_tokens, refs_tokens, n_gram=3)
        b4 = bleu_score(preds_tokens, refs_tokens, n_gram=4)
        return {"R1": rouge_results["rouge_1/f_score"],
                "R2": rouge_results["rouge_2/f_score"],
                "RL": rouge_results["rouge_l/f_score"],
                "bleu1": b1,
                "bleu2": b2,
                "bleu3": b3,
                "bleu4": b4}

    @staticmethod
    def _evaluate_all_(rpreds, rrefs, epreds, erefs):
        """
        Evaluate the rating predictions (rpreds, rrefs) and explanaiton generations(epreds, erefs)both.
        Please keep the inputs are in correct type.
        param: rpreds and rrefs in shape (N, 1) or (N) (keep same)
        param: epreds and erefs are list of list of ids.
        """
        ratings_scores = compute_mae_rmse(rpreds, rrefs)
        rouge_results = rouge_score(epreds, erefs)
        preds_tokens = [tokenize(sentence) for sentence in epreds]
        refs_tokens = [tokenize(sentence) for sentence in erefs]
        b1 = bleu_score(preds_tokens, refs_tokens, n_gram=1)
        b2 = bleu_score(preds_tokens, refs_tokens, n_gram=2)
        b3 = bleu_score(preds_tokens, refs_tokens, n_gram=3)
        b4 = bleu_score(preds_tokens, refs_tokens, n_gram=4)
        return {"MAE": ratings_scores[0],
                "RMSE": ratings_scores[1],
                "R1": rouge_results["rouge_1/f_score"],
                "R2": rouge_results["rouge_2/f_score"],
                "RL": rouge_results["rouge_l/f_score"],
                "bleu1": b1,
                "bleu2": b2,
                "bleu3": b3,
                "bleu4": b4}

    def eval_batch(self, batch, device, tokenizer, tasks):
        """
        This is useful when you hope to examine single sample, or batched sample.
        tasks = "recommend", "generate", or ["recommend", "generate"]
        """
        self.eval()
        self = self.to(device)
        with torch.no_grad():
            input_ids, tgt_input, tgt_output, rating = self.gather(batch, device)
            if tasks == "generate":
                output_ids = self.generate(input_ids, device=device)  # return is tensor in shape (N, max_len).
                pred_explan = tokenizer.decode(output_ids, return_sentence=True)["sentences"]
                target_explan = tokenizer.decode(tgt_output, return_sentence=True)["sentences"]
                score = self._evaluate_text_(pred_explan, target_explan)

            elif tasks == "recommend":
                pred_rating = self.recommend(input_ids, device=device)  # return is tensor in shape (N) or (N,1)
                score = self._evaluate_rating_(pred_rating, rating)
            else:
                # both
                output_ids = self.generate(input_ids, device=device)  # return is tensor in shape (N, max_len).
                pred_explan = tokenizer.decode(output_ids, return_sentence=True)["sentences"]
                target_explan = tokenizer.decode(tgt_output, return_sentence=True)["sentences"]
                pred_rating = self.recommend(input_ids, device=device)  # return is tensor in shape (N) or (N,1)
                score = self._evaluate_all_(pred_rating, rating, pred_explan, target_explan)
        return score

    def evalIter(self, dataloader, device, tokenizer, tasks):
        """
        We follow "all-in-once" evlauating style. That is, we first gather all batched data into a "big" batch, then compute.
        The reason we do so is that looping computations on every batch would cause more time cost especially when the metric to be computed is expensive, such as ROUGE.
        tasks: "generate", "recommend", or ["generate", "recommend"]
        """
        self.eval()
        self = self.to(device)
        pred_ratings = []
        pred_explans = []
        target_ratings = []
        target_explans = []
        with torch.no_grad():
            for batch in tqdm(dataloader, leave=False):
                input_ids, tgt_input, tgt_output, rating = self.gather(batch, device)
                if tasks == "generate":
                    output_ids = self.generate(input_ids, device=device)  # return is tensor in shape (N, max_len).
                    pred_explan = tokenizer.decode(output_ids, return_sentence=True)["sentences"]
                    target_explan = tokenizer.decode(tgt_output, return_sentence=True)["sentences"]
                    pred_explans.extend(pred_explan)
                    target_explans.extend(target_explan)

                elif tasks == "recommend":
                    pred_rating = self.recommend(input_ids, device=device)  # return is tensor in shape (N) or (N,1)
                    pred_ratings.append(pred_rating)
                    target_ratings.append(rating)

                else:
                    # both
                    output_ids = self.generate(input_ids, device=device)  # return is tensor in shape (N, max_len).
                    pred_explan = tokenizer.decode(output_ids, return_sentence=True)["sentences"]
                    target_explan = tokenizer.decode(tgt_output, return_sentence=True)["sentences"]
                    pred_explans.extend(pred_explan)
                    target_explans.extend(target_explan)

                    pred_rating = self.recommend(input_ids, device=device)  # return is tensor in shape (N) or (N,1)
                    pred_ratings.append(pred_rating)
                    target_ratings.append(rating)

            if tasks == "generate":
                scores = self._evaluate_text_(pred_explans, target_explans)
            elif tasks == "recommend":
                pred_ratings = torch.cat(pred_ratings)
                target_ratings = torch.cat(target_ratings)
                scores = self._evaluate_rating_(pred_ratings, target_ratings)
            else:
                pred_ratings = torch.cat(pred_ratings)
                target_ratings = torch.cat(target_ratings)
                scores = self._evaluate_all_(pred_ratings, target_ratings, pred_explans, target_explans)
        return scores


# def greedy_encode_decode(model=None, input_ids=None, device="cpu", start=2, max_seq_length=20):
#     """
#     This greedy algorithm may only apply for encoder-decoder style model.
#     """
#     # conver to eval model and to device
#     model.eval()
#     model = model.to(device)
#     if torch.is_tensor(input_ids):
#         batch_size = input_ids.shape[0]
#     else:
#         batch_size = input_ids[0].shape[0]  # input_ids may be a tuple, e.g., (user_ids, item_ids)
#     with torch.no_grad():
#         memory = model.encode(input_ids)  # in shape (N, seqlen, emsize)
#         decoder_input_ids = torch.zeros((batch_size, 1)).fill_(start).long().to(device)
#         for i in range(max_seq_length):
#             hidden = model.decode(decoder_input_ids, memory)  # in shape (N, seqlen, ntoken)
#             output_id = hidden[:, -1, :].topk(1).indices
#             decoder_input_ids = torch.cat([decoder_input_ids, output_id], dim=-1)
#     return decoder_input_ids[:, 1:]

# def topk_encode_decode(model=None, input_ids=None, k=10, temp=0.7, device="cpu", start=2, max_seq_length=20, seed=3407):
#     """
#       This tok-k sampling algorithm may only apply for encoder-decoder style model.
#     """
#     # conver to eval model and to device
#     torch.manual_seed(seed)
#     model.eval()
#     model = model.to(device)
#     if torch.is_tensor(input_ids):
#         batch_size = input_ids.shape[0]
#     else:
#         batch_size = input_ids[0].shape[0]  # input_ids may be a tuple, e.g., (user_ids, item_ids)
#     with torch.no_grad():
#         memory = model.encode(input_ids)  # in shape (N, seqlen, emsize)
#         decoder_input_ids = torch.zeros((batch_size, 1)).fill_(start).long().to(device)
#         for i in range(max_seq_length):
#             hidden = model.decode(decoder_input_ids, memory)  # in shape (N, seqlen, ntoken)
#             # softmax with temperature to sharpen the distritbuion.
#             weights, indices = (hidden[:,-1,:]/0.7).softmax(dim=-1).topk(k)
#             returned_idx = torch.multinomial(weights, 1)        # sampling among the top-k elements by normalized weights.
#             output_id = torch.gather(indices, 1, returned_idx)  # output_id = hidden[:, -1, :].topk(1).indices
#             decoder_input_ids = torch.cat([decoder_input_ids, output_id], dim=-1)
#     return decoder_input_ids[:, 1:]


class Trainer():
    def __init__(self,
                 batch_size=128,
                 max_epochs=100,
                 accelerator="cpu",
                 optimizer_method="Adam",
                 lr=1e-2,
                 weight_decay=1e-15,
                 scheduler_method="StepLR",
                 warmup=False,
                 max_clip_norm=5,
                 logging=None,  # Str type, name for logging file.  update: + tensorboard visualization.
                 save_model=None,  # Str type, name for saved model file.
                 save_every_epoch=False,
                 use_amp=False,
                 early_stop=True,
                 valid_metrics=["loss"],
                 efficient_valid=True,
                 accum_step=1):

        """
            Example:
                >>> model = ...
                >>> trainer = Trainer()
                >>> trainer.fit(model)
        """
        self.batch_size = batch_size  # used only for recording.
        self.max_epochs = max_epochs
        self.accelerator = accelerator
        self.optimizer_method = optimizer_method
        self.lr = lr
        self.weight_decay = weight_decay
        self.scheduler_method = scheduler_method
        self.warmup = warmup
        self.max_clip_norm = max_clip_norm
        self.use_amp = use_amp
        self.early_stop = early_stop
        self.accum_step = accum_step
        if logging:
            # we only record some often changing settings.
            kargs = {"batch_size": batch_size,
                     "optimizer": optimizer_method,
                     "learning_rate": lr,
                     "use_amp": use_amp,
                     "warmup": warmup,
                     "saved": save_model,
                     "optimizer_method": optimizer_method,
                     "scheduler_method": scheduler_method,
                     "early_stop": early_stop,
                     "accum_step": accum_step
                     }
            self.logger = Logger("./logs/" + logging, **kargs)  # create log file and first save hyper-setttings.
            self.writer = SummaryWriter()
        else:
            self.logger = logging
        self.save_model = save_model
        self.save_every_epoch = save_every_epoch
        self.valid_metrics = valid_metrics
        self.efficient_valid = efficient_valid

    @staticmethod
    def _get_optimizer_(model, optimizer_method="Adam", lr=1e-2, weight_decay=1e-5):
        # Example:
        # >>> optimizer = trainer._get_optimizer_(model)
        if optimizer_method == "Adadelta":
            optimizer = optim.Adadelta(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,
                                       weight_decay=weight_decay)
        elif optimizer_method == "Adagrad":
            optimizer = optim.Adagrad(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,
                                      weight_decay=weight_decay)
        elif optimizer_method == "Adam":
            optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,
                                   weight_decay=weight_decay)
        elif optimizer_method == "Adamax":
            optimizer = optim.Adamax(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,
                                     weight_decay=weight_decay)
        elif optimizer_method == "AdamW":
            optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,
                                    weight_decay=weight_decay)
        elif optimizer_method == "RMSProp":
            optimizer = optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,
                                      weight_decay=weight_decay)
        elif optimizer_method == "RAdam":
            optimizer = optim.RAdam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,
                                    weight_decay=weight_decay)
        elif optimizer_method == "Rprop":
            optimizer = optim.Rprop(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,
                                    weight_decay=weight_decay)
        elif optimizer_method == "SGD":
            optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,
                                  weight_decay=weight_decay)
        else:
            raise Exception('Please check if your optimizer is correct!')
        return optimizer

    @staticmethod
    def _get_scheduler_(optimizer, scheduler_method="StepLR", early_stop=True, max_epochs=100, lr=1e-1):
        """
        The scheduler strategy differs depends of whether we are using early stop.
             We recommend:
                  - if you set early stop True, then use "StepLR" scheduler is better because it can be easily adjusted compared to cosine scheduler.
                  - if you don't choose early stop, then "StepLR" and "CosineLR" scheduler are both approapriate.
        """
        if early_stop:
            if scheduler_method == "StepLR":
                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)  # change instantly
            else:
                scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs / 10,
                                                                 eta_min=1e-6)  # you need to manually change the learning rate of optimizer smaller. (1/10 times.)
        else:
            if scheduler_method == "StepLR":
                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=max_epochs / 10,
                                                      gamma=0.5)  # e.g., Automatically changes 10 times if total 100.
            elif scheduler_method == "CosineLR":
                scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs / 10,
                                                                 eta_min=1e-6)  # Automatically changes 10 times if total 100.
            else:
                raise Exception("Please check your scheduler method")

        # set warmup. Note that the warmup stragegy won't be affected by whether early stop.
        warm_up_steps = int(max(1, max_epochs / 20))  # .e.g, takes 5 steps to warmup if total 100.
        warm_up_ratio = (lr - 1e-99) / warm_up_steps
        return scheduler, warm_up_steps, warm_up_ratio

    def _log_and_print_(self, epoch, valid_results, data_type):
        # log and print
        if self.logger:
            self.writer.add_scalar('Loss/valid', valid_results["loss"], epoch)
            self.logger.add_log(f"------------------------- epoch: {epoch} -------------------------")
            self.logger.add_log(data_type)
            for key in valid_results.keys():
                self.logger.add_log(f"{key}: {valid_results[key]}")
        # print to console
        print(f"------------------------- epoch: {epoch} -------------------------")
        print(data_type)
        for key in valid_results.keys():
            print(f"{key}: {valid_results[key]}")

    def _save_model_(self, model):
        torch.save(model.state_dict(), "./saved/" + self.save_model + "_dict")

    def _save_model_every_epoch_(self, model, epoch):
        dir_path = "./saved/epochs_" + self.save_model
        if not os.path.exists(dir_path):
            os.mkdir(dir_path)
        save_path = os.path.join(dir_path, str(epoch))
        torch.save(model.state_dict(), save_path + "_dict")

    def _early_stop_(self, endure_times, best_loss, current_loss):
        trigger = False  # when trigger True, we need to scheduler.step()
        if current_loss < best_loss:
            best_loss = current_loss
        else:
            if current_loss > best_loss:
                endure_times += 1
                trigger = True
                print(f"check early stopping...current endurance {endure_times}")
                if self.logger:
                    self.logger.add_log(f"check early stopping...current endurance {endure_times}")
        return endure_times, current_loss, trigger

    def fit(self, model, train_dataloader=None, valid_dataloader=None):
        if valid_dataloader is None:
            print("Warning: validation dataset is None. The training data is used as validation set. ")
            valid_dataloader = train_dataloader
        print("Start training...")
        print(f"The total number of parameters is: {count_param(model)}")
        """
        Note that early_stop setting and save_model are indepedent. Feel free to change either.
        """
        if self.early_stop:  # we set endurence time to 5. feel free to change it to other values, e.g., 10.
            endure_times = 0
            max_endurance = 5
            best_loss = float("inf")
        if self.save_model:
            save_loss = float("inf")
        model = model.to(self.accelerator)
        optimizer = Trainer._get_optimizer_(model, self.optimizer_method, self.lr, self.weight_decay)
        scheduler, warm_up_steps, warm_up_ratio = Trainer._get_scheduler_(optimizer, self.scheduler_method,
                                                                          self.early_stop, self.max_epochs, self.lr)
        for epoch in range(self.max_epochs):
            # warm up setttings.  If not use warmup, simply set the warm_up_steps to 0.
            if not self.warmup:
                warm_up_steps = 0
            if epoch < warm_up_steps:  # linearly warm up.  Currently we only support one group parameters of optimizer.
                optimizer.param_groups[0]["lr"] = epoch * warm_up_ratio + (1e-99)  # epsilon smoothing
            if epoch == warm_up_steps:  # recover from the original pre-set lr.
                optimizer.param_groups[0]["lr"] = self.lr
            print("current learning rate:", optimizer.param_groups[0]["lr"])
            if self.logger:
                self.logger.add_log(f"current learning rate: {optimizer.param_groups[0]['lr']}")
            model.trainIter(dataloader=train_dataloader,
                            device=self.accelerator,
                            optimizer=optimizer,
                            use_amp=self.use_amp,
                            max_clip_norm=self.max_clip_norm,
                            accum_step=self.accum_step)
            train_results = model.validIter(dataloader=train_dataloader,
                                            device=self.accelerator,
                                            metrics=self.valid_metrics,
                                            efficient_valid=self.efficient_valid)
            valid_results = model.validIter(dataloader=valid_dataloader,
                                            device=self.accelerator,
                                            metrics=self.valid_metrics,
                                            efficient_valid=False)  # for better learning, we set efficient_valid here, False.
            self._log_and_print_(epoch, train_results, "on train data")
            self._log_and_print_(epoch, valid_results, "on valid data")
            # (if) save model if current loss is the lowest. -> save the last one.
            if self.save_model:
                if self.save_every_epoch:
                    self._save_model_every_epoch_(model, epoch)
                else:
                    self._save_model_(model)
            # (if) early stopping: default endurance five times.
            if self.early_stop:
                endure_times, best_loss, trigger = self._early_stop_(endure_times, best_loss, valid_results["loss"])
                if endure_times >= max_endurance:
                    if self.logger:
                        self.logger.add_log("Eearly Stopping!")
                    print("Eearly Stopping!")
                    break
                elif trigger:
                    if epoch > warm_up_steps:
                        if optimizer.param_groups[0][
                            "lr"] < 5e-7:  # when learning rate becomes too small, we also early stop.
                            break
                        scheduler.step()
                    else:
                        endure_times = 0  # in warmup steps, prevent from early stop.
                else:
                    endure_times = 0  # only k consecutive times of no progress we early stop; in other words, once trigger False, we restart the endurance sign from 0.

            # adjust the learning rate by scheduler only after the warmup steps and no early stop.
            if not self.early_stop and epoch > warm_up_steps:
                scheduler.step()


class Evaluator():
    """
    This class is used for evaluating results based on specified metrics. Inspired by Transformers evaluate lib.
        Example:
            >>> X = torch.tensor([1,0,0,1])
            >>> y = torch.tensor([0,0,0,1])
            >>> evaluator = Evaluator()
            >>> evaluator.compute(X,y)
    """

    def __init__(self, metric="precision"):
        self.metric = metric
        self.preds = []
        self.refs = []

    def add_batch(self, preds, refs):
        """
        Incrementally add batched data and then call .compute() to compute the results in a "all-in-one" manner.
        :param preds:
        :param refs:
        :return:
            Example:
            >>> for batch in dataloader:
            >>>       X, y = batch
            >>>       evaluator.add(X,y)
            >>> evaluator.compute()
        """
        if isinstance(preds, list) and isinstance(refs, list):  # for list data, we extend to a long list.
            self.preds.extend(preds)
            self.refs.extend(refs)
        elif torch.is_tensor(preds) and torch.is_tensor(
                refs):  # for tensor data, we append and then call torch.cat into the same type as batch data.
            self.preds.append(preds)
            self.refs.append(refs)
        else:
            raise Exception("only support tensor or list type")

    def compute(self, preds=None, refs=None):  # you can evluate directly by specifying the input.
        if preds is None and refs is None:  # After .add().  This is used for iterating over the whole dataset.
            if torch.is_tensor(self.preds[0]) and torch.is_tensor(self.refs[0]):
                preds = torch.cat(self.preds)
                refs = torch.cat(self.refs)
            else:
                preds = self.preds
                refs = self.refs
        result = self._compute_(preds, refs)
        return result

    def _compute_(self, preds, refs):
        if self.metric == "precision":
            return self._compute_precision_(preds, refs)

    @staticmethod
    def _compute_precision_(preds, refs):
        # preds and refs should be tensors in shape (N)
        if isinstance(preds, list) or isinstance(refs, list):
            preds = torch.tensor(preds)
            refs = torch.tensor(refs)
        else:  # tensor type
            preds = preds.reshape(-1)
            refs = refs.reshape(-1)
        precision = (preds == refs).sum() / preds.shape[0]
        return precision


####################################################################################################
# Training-related utitlies:
#        1. Train.
#        2. evaluate: BLEU score, ROUGE score, MAE & RMSE.

def compute_mae_rmse(pred, target):
    """
    Returns MAE and RMSE values.
        Example:
            >>> pred = torch.randn(100)
            >>> ref = torch.randn(100)
            >>> MAE, RMSE = compute_mae_rmse(pred, ref)
    """
    if isinstance(pred, list) or isinstance(target, list):
        pred = torch.Tensor(pred)
        target = torch.Tensor(target)
    with torch.no_grad():
        mae = F.l1_loss(pred, target)
        rmse = F.mse_loss(pred, target) ** (1 / 2)
    return round(mae.item(), 4), round(rmse.item(), 4)


def rouge_score(generated, references):
    """
    A list of lists of tokens, the same format as bleu.
        Example:
            >>> generated = [[52, 529, 3345, 444], [12, 32, 432, 2]]
            >>> references = [[43, 24, 53, 666], [53, 43, 3, 333, 444]]   # two different sentences
    """
    score = rouge(generated, references)
    rouge_s = {k: (v * 100) for (k, v) in score.items()}
    '''
    "rouge_1/f_score": rouge_1_f,
    "rouge_2/f_score": rouge_2_f,
    "rouge_l/f_score": rouge_l_f,
    '''
    return rouge_s


def string_subtract(str1, str2):
    str1 = str1.replace(str2, "")
    return str1


def bleu_score(generated, references, n_gram=4, smooth=False):
    """a list of lists of tokens"""
    formatted_ref = [[ref] for ref in references]
    try:
        bleu_s, _, _, _, _, _ = compute_bleu(formatted_ref, generated, n_gram, smooth)
    except:
        bleu_s = 0
    return bleu_s * 100


# compute NDCG

# copy from: https://github.com/haowei01/pytorch-examples/blob/master/ranking/metrics.py
class DCG(object):
    def __init__(self, k=10, gain_type='exp2'):
        """
        :param k: int DCG@k
        :param gain_type: 'exp2' or 'identity'
        """
        self.k = k
        self.discount = self._make_discount(256)
        if gain_type in ['exp2', 'identity']:
            self.gain_type = gain_type
        else:
            raise ValueError('gain type not equal to exp2 or identity')

    def evaluate(self, targets):
        """
        :param targets: ranked list with relevance
        :return: float
        """
        gain = self._get_gain(targets)
        discount = self._get_discount(min(self.k, len(gain)))
        return np.sum(np.divide(gain, discount))

    def _get_gain(self, targets):
        t = targets[:self.k]
        if self.gain_type == 'exp2':
            return np.power(2.0, t) - 1.0
        else:
            return t

    def _get_discount(self, k):
        if k > len(self.discount):
            self.discount = self._make_discount(2 * len(self.discount))
        return self.discount[:k]

    @staticmethod
    def _make_discount(n):
        x = np.arange(1, n + 1, 1)
        discount = np.log2(x + 1)
        return discount


class NDCG(DCG):
    """
    Example:
        >>> target_score = [4,3,5]  #ranked list with relevance score
        >>> ndcg = NDCG(k=10)
        >>> score = ndcg.evaluate(target_score)
    """

    def __init__(self, k=10, gain_type='exp2'):
        """
        :param k: int NDCG@k
        :param gain_type: 'exp2' or 'identity'
        """
        super(NDCG, self).__init__(k, gain_type)

    def evaluate(self, targets):
        """
        :param targets: ranked list with relevance
        :return: float
        """
        dcg = super(NDCG, self).evaluate(targets)
        ideal = np.sort(targets)[::-1]
        idcg = super(NDCG, self).evaluate(ideal)
        return dcg / idcg

    def maxDCG(self, targets):
        """
        :param targets: ranked list with relevance
        :return:
        """
        ideal = np.sort(targets)[::-1]
        return super(NDCG, self).evaluate(ideal)


def compute_kl(dist1, dist2):
    """
    KL divergence measures how similar two distributions are. 1 means no similar, 0 means same.
    dist1, dist2 in shape (N, dim), while dist2 is target distribution.
    Example:
        >>> dist1, dist2 = torch.randn(2,5).softmax(-1), torch.randn(2,5).softmax(-1)
        >>> compute_kl(dist1, dist2)
    """
    length1 = len(dist1)
    length2 = len(dist2)
    assert length1 == length2, "distribution length not match!"
    return ((dist2.log() - dist1.log()) * dist2).sum(-1).mean()


def compute_rbf_kernel(X1, X2, gamma=1):
    """
    Compute the rbf kernel between two matrix.
    Example:
        >>> X1 = torch.randn(20, 64)
        >>> X2 = torch.randn(10,64)
        >>> kernel_score = compute_rbf_kernel(X1, X2)  # in shape (20,10), similar to cosine simiarity.
    """
    kernel_scores = torch.zeros(X1.shape[0], X2.shape[0])
    for i in range(kernel_scores.shape[0]):
        for j in range(kernel_scores.shape[1]):
            kernel_scores[i][j] = torch.exp(-gamma * ((X1[i] - X2[j]) ** 2).sum())
    return kernel_scores


def LDA_from_scratch():
    """
    Returns:
    """


class VAE_from_scratch(JoModule):
    def __init__(self, embed_size=128):
        super().__init__()
        self.embed_size = embed_size
        self.encoder = nn.Sequential(
            nn.Linear(784, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU())
        self.encodeu = nn.Linear(512, embed_size)
        self.encodes = nn.Linear(512, embed_size)
        self.decoder = nn.Sequential(nn.Linear(embed_size, 512),
                                     nn.ReLU(),
                                     nn.Linear(512, 512),
                                     nn.ReLU(),
                                     nn.Linear(512, 784),
                                     nn.Sigmoid())
        self.loss_fn = nn.MSELoss()
        self.init_weight()

    def init_weight(self):
        self.apply(xavier_normal_initialization)

    def gather(self, batch, device):
        X, y = batch[0], batch[1]
        X = X.view(-1, 784).to(device)
        y = y.to(device)
        return X, y

    def forward(self, X):
        pred_u = self.encodeu(self.encoder(X))  # in shape (N, embed_size)
        pred_s = torch.exp(self.encodes(self.encoder(X)))  # in shape (N, embed_size)
        z = pred_u + torch.randn_like(pred_s) * pred_s  # in shape (N, embed_size)
        pred_x = self.decoder(z)
        return pred_x, z, pred_u, pred_s

    def training_step(self, batch, device):
        X, y = self.gather(batch, device)  # in shape (N, 784), (N)
        pred_x, z, pred_u, pred_s = self.forward(X)  # in shape (N, 784), (N, embed_size)
        loss_re = self.loss_fn(pred_x, X)
        loss_kl = (pred_s ** 2 + pred_u ** 2 - torch.log(pred_s) - 1 / 2).sum()
        loss = loss_re + 1e-4 * loss_kl
        return loss

    def validation_step(self, batch, device, metrics):
        # X, y = self.gather(batch, device)  # in shape (N, 784), (N)
        # pred_x, z, pred_u, pred_s = self.forward(X)        # in shape (N, 784), (N, embed_size)
        # loss_re = self.loss_fn(pred_x, X)
        loss = self.training_step(batch, device)
        return {"loss": loss}

def is_convex_or_concave(f, domain):
    """
    To determine if a function is convex function, convave, or neither. It works by checking second-derivative.
    Example:
        >>> def f(x):
                return x ** 3
        >>> domain = [-5, 5]
        >>> is_convex_or_concave(f, domain)
    """
    x = np.linspace(domain[0], domain[1], 1000)
    y = f(x)
    d2y = np.gradient(np.gradient(y, x), x)
    if all(d2y >= 0):
        return "convex"
    elif all(d2y <= 0):
        return "concave"
    else:
        return "neither"


def check_JesenInequality(f):
    """
    Check JesenInequality holds for a given function. It holds for all convex function.
    """
    x = torch.randn([1, 10])
    w = torch.randn([10, 1]).softmax(0)
    f_expectation = f(x @ w)
    expectation_f = f(x) @ w
    if f_expectation <= expectation_f:
        print("Jensen's Inequality holds for the convex function.")
    else:
        print("Not hold, please check if convex function.")


### The following creates the probablistic classes.
def check_dist(dist):
    print("mean: ", dist.mean)
    print("stddev: ", dist.stddev)
    print("variance: ", dist.variance)
    print("mode: ", dist.mode)
    print("entropy: ", dist.entropy())
    print("perplexity: ", dist.perplexity())


def draw_dist(dist):
    """
    Simply plot a scatter figure for a pre-defined distribution.
    Example:
        >>> dist = torch.distirbutions.Normal(loc=0, scale=1)
        >>> plot_dist(dist)
    """
    samples = dist.sample(sample_shape=torch.Size([100]))
    prob = dist.log_prob(samples)
    plt.scatter(samples, prob)


class JoDistribution():
    """
    Create a parent class for various families of distribution.
    You only need to rewrite "__init__" (hyper-parameters & lowerbound, upperbound, length, guess) and "rsample" methods.
    length denotes the number of distributions.
    guess is used for find the icdf values via optimize.fsolve function.
    """

    def __init__(self):
        self.lowerbound = -30
        self.upperbound = 30
        self.length = 1  # number of distributions. e.g., if the input loc and scale are tensors, then self.length = len(scale)
        self.guess = 0

    @property
    def mean(self):
        # intergral p(x)*x.
        means = []
        for i in range(self.length):
            mean, _ = integrate.quad(lambda x: self.pdf(x)[i] * x, self.lowerbound, self.upperbound)
            means.append(mean)
        means = torch.tensor(means)
        return means

    @property
    def variance(self):
        # intergral p(x)*(x-mu)^2, or E(x^2) - E(x)
        means_x2 = []
        for i in range(self.length):
            mean_x2, _ = integrate.quad(lambda x: self.pdf(x)[i] * x ** 2, self.lowerbound, self.upperbound)
            means_x2.append(mean_x2)
        variance = torch.tensor(means_x2) - self.mean ** 2
        return variance

    @property
    def stddev(self):
        return self.variance ** (1 / 2)

    @property
    def mode(self):
        # find the maximize.
        modes = []
        for i in range(self.length):
            mode = fminbound(lambda x: -self.pdf(x)[i], self.lowerbound, self.upperbound)
            modes.append(mode)
        modes = torch.tensor(modes)
        return modes

    def entropy(self):
        # intergral -p(x) * logp(x)
        entropys = []
        for i in range(self.length):
            entropy, _ = integrate.quad(lambda x: self.pdf(x)[i] * self.pdf(x)[i].log(), self.lowerbound,
                                        self.upperbound)
            entropys.append(-entropy)
        entropys = torch.tensor(entropys)
        return entropys

    def perplexity(self):
        entropy = self.entropy()
        return torch.exp(entropy)

    def log_prob(self, x):
        return self.pdf(x).log()

    def cdf(self, x):
        """
        x is a tensor.
        Example:
           >>> x = torch.tensor([1,2,3]). x should be (1), (length) or (k, self.length)
           >>> dist.cdf(x)
        """
        if len(x.shape) == 1:
            if x.shape[0] == 1:
                outs = []
                for i in range(self.length):
                    out, _ = integrate.quad(lambda x: self.pdf(x)[i], self.lowerbound, x)
                    outs.append(out)
                outs = torch.tensor(outs)

            elif self.length == 1:
                outs = []
                for i in range(len(x)):
                    out, _ = integrate.quad(lambda x: self.pdf(x), self.lowerbound, x[i])
                    outs.append(out)

            else:
                outs = []
                for i in range(self.length):
                    out, _ = integrate.quad(lambda x: self.pdf(x)[i], self.lowerbound, x[i])
                    outs.append(out)
                outs = torch.tensor(outs)

        elif len(x.shape) == 2:
            outs = []
            for i in range(x.shape[0]):
                for j in range(self.length):
                    out, _ = integrate.quad(lambda x: self.pdf(x)[j], self.lowerbound, x[i, j])
                    outs.append(out)

        outs = torch.tensor(outs).view(-1, self.length)
        return outs

    def icdf(self, p):
        """
        p is the same shape as x in "cdf" method.
        Example:
            >>> p =
            >>> dist.icdf(x)
        """
        if len(p.shape) == 1:
            if p.shape[0] == 1:
                outs = []
                for i in range(self.length):
                    cdf_lambda = lambda x: integrate.quad(lambda x: self.pdf(x)[i], self.lowerbound, x)[0]
                    out = fsolve(lambda x: cdf_lambda(x) - p, self.guess)
                    outs.append(out.item())
                outs = torch.tensor(outs)

            elif self.length == 1:
                outs = []
                for i in range(len(p)):
                    cdf_lambda = lambda x: integrate.quad(lambda x: self.pdf(x), self.lowerbound, x)[0]
                    out = fsolve(lambda x: cdf_lambda(x) - p[i], self.guess)
                    outs.append(out.item())
                outs = torch.tensor(outs)

            else:
                outs = []
                for i in range(self.length):
                    cdf_lambda = lambda x: integrate.quad(lambda x: self.pdf(x)[i], self.lowerbound, x)[0]
                    out = fsolve(lambda x: cdf_lambda(x) - p[i], self.guess)
                    outs.append(out)
                outs = torch.tensor(outs).view(-1)

        elif len(p.shape) == 2:
            outs = []
            for i in range(p.shape[0]):
                for j in range(self.length):
                    cdf_lambda = lambda x: integrate.quad(lambda x: self.pdf(x)[j], self.lowerbound, x)[0]
                    out = fsolve(lambda x: cdf_lambda(x) - p[i, j], self.guess)
                    outs.append(out)
            outs = torch.tensor(outs).view(-1, self.length)
        return outs

    def sample(self, sample_shape):
        with torch.no_grad():
            return self.rsample(sample_shape)

    def rsample(self, sample_shape):
        p = torch.rand(sample_shape)
        x = self.icdf(p)
        return x


class JoUniform(JoDistribution):
    """
    Create a uniform distribution instance.
    Example:
        >>> dist = JoUniform(low=1, high=5)
    """

    def __init__(self, low, high):
        if not torch.is_tensor(low):
            low = torch.tensor(low, dtype=torch.float32)
        if not torch.is_tensor(high):
            high = torch.tensor(high, dtype=torch.float32)
        self.low = low
        self.high = high
        self.pdf = lambda x: torch.tensor([1 / (self.high - self.low)])

        self.lowerbound = self.low.item()
        self.upperbound = self.high.item()
        self.length = len(self.low)
        self.guess = 0


class JoNormal(JoDistribution):
    """
    Create a normal distribution instance.
    Example:
        >>> dist = JoNormal(loc=0, scale=1)
    """

    def __init__(self, loc, scale):
        if not torch.is_tensor(loc):
            loc = torch.tensor(loc, dtype=torch.float32)
        if not torch.is_tensor(scale):
            scale = torch.tensor(scale, dtype=torch.float32)
        self.loc = loc
        self.scale = scale
        self.pdf = lambda x: (1 / torch.sqrt(2 * torch.pi * self.scale ** 2)) * torch.exp(
            -(x - self.loc) ** 2 / (2 * self.scale ** 2))

        self.lowerbound = -10
        self.upperbound = 10
        self.length = len(self.loc)
        self.guess = 0

class JoMultivariateNormal(JoDistribution):
    """
    Create a Multivariate Normal distribution instance.
    """
    def __init__(self, rate):
        raise Exception("Not implemented yet.")

class JoPoisson(JoDistribution):
    """
    Create a Poisson distribution instance.
    """
    def __init__(self, rate):
        if not torch.is_tensor(rate):
            rate = torch.tensor(rate, dtype=torch.float32)
        self.rate = rate
        self.pdf = lambda x: self.rate.pow(x) * torch.exp(-self.rate)/math.factorial(x)


class Gumbel(JoDistribution):
    """
    Create a Gumbel distribution instance.
    """
    def __init__(self, loc, scale):
        if not torch.is_tensor(loc):
            loc = torch.tensor(loc, dtype=torch.float32)
        if not torch.is_tensor(scale):
            scale = torch.tensor(scale, dtype=torch.float32)
        self.loc = loc
        self.scale = scale
        self.pdf = lambda x: (1 / self.scale) * math.exp(-((x - self.loc) / self.scale + math.exp(-(x - self.loc) / self.scale)))

class JoBernoulli(JoDistribution):
    """
    Create a Bernoulli distribution instance.
    """
    def __init__(self, prob):
        if not torch.is_tensor(prob):
            prob = torch.tensor(prob, dtype=torch.float32)
        self.prob = prob
        self.pdf = lambda x: self.prob.pow(x) * (1-self.prob).pow(1-x)

class JoBinomial(JoDistribution):
    """
    Create a Binomial distribution instance.
    """
    def __init__(self, k, prob):
        if not torch.is_tensor(prob):
            prob = torch.tensor(prob, dtype=torch.float32)
        if not torch.is_tensor(k):
            k = torch.tensor(k)     # number of trial
        self.prob = prob
        self.k = k
        self.pdf = lambda x: math.comb(self.k, x) * self.prob.pow(x) * (1-self.prob).pow(self.k-x)


class JoMultinomial(JoDistribution):
    """
    Create a Multinomial distribution instance.
    """
    def __init__(self, k, prob):
        if not torch.is_tensor(prob):
            prob = torch.tensor(prob, dtype=torch.float32)
        if not torch.is_tensor(k):
            k = torch.tensor(k)     # number of trial
        self.prob = prob
        self.k = k
        self.pdf = lambda x: math.factorial(self.k)/(torch.tensor([(math.factorial(x[i])) for i in range(len(x))])).prod() * torch.tensor([self.prob[i].pow(x[i])]).prod()

class JoCategorical(JoDistribution):
    """
    Create a categorical distribution instance. The probability mass function is p(x) = prob^i. Kind of similar to Bernoulli + Multinomial.
    """
    def __init__(self, rate):
        raise Exception("Not implemented yet.")

class JoNegativeBinomial(JoDistribution):
    """
    Create a Negative Binomial distribution instance.
    """
    def __init__(self, r, prob):
        if not torch.is_tensor(prob):
            prob = torch.tensor(prob, dtype=torch.float32)
        if not torch.is_tensor(r):
            r = torch.tensor(r)     # number of required failures
        self.prob = prob
        self.r = r
        self.pdf = lambda x: math.factorial(x+self.r-1, self.r-1) * self.p.pow(x) * (1-self.p).pow(r)


class JoBeta(JoDistribution):
    """
    Create a Beta distribution instance.
    """
    def __init__(self, alpha, beta):
        import scipy.special as sp
        if not torch.is_tensor(alpha):
            alpha = torch.tensor(alpha, dtype=torch.float32)
        if not torch.is_tensor(beta):
            beta = torch.tensor(beta)     # number of required failures
        self.alpha = alpha
        self.beta = beta
        Beta_fun = lambda alpha, beta: sp.gamma(alpha) * sp.gamma(beta) / sp.gamma(alpha + beta)
        self.pdf = lambda x: x.pow(self.alpha-1) * (1-x).pow(self.beta-1) / Beta_fun(self.alpha, self.beta)

class JoGamma(JoDistribution):
    """
    Create a gamma distribution instance.
    """
    def __init__(self, alpha, beta):
        import scipy.special as sp
        if not torch.is_tensor(alpha):
            alpha = torch.tensor(alpha, dtype=torch.float32)
        if not torch.is_tensor(beta):
            beta = torch.tensor(beta)     # number of required failures
        self.alpha = alpha
        self.beta = beta
        self.pdf = lambda x: (self.beta.pow(self.alpha) / sp.gamma(self.alpha)) * x.pow(self.alpha - 1) * math.exp(-self.beta * x)


class JoDirichlet(JoDistribution):
    """
    Create a dirichelet distribution instance.
    """
    def __init__(self, alpha):
        import scipy.special as sp
        if not torch.is_tensor(alpha):
            alpha = torch.tensor(alpha, dtype=torch.float32)
        self.alpha = alpha
        coef = sp.gamma(self.alpha.sum())/torch.tensor([sp.gamma(self.alpha[i]) for i in range(len(self.alpha))]).prod()
        self.pdf = lambda x:  coef * torch.tensor([x[i].pow(self.alpha[i]-1) for i in range(len(self.alpha))]).prod()


class JoExponential(JoDistribution):
    """
    Create a Exponenetial distribution instance.
    """
    def __init__(self, rate):
        if not torch.is_tensor(rate):
            rate = torch.tensor(rate, dtype=torch.float32)
        self.rate = rate
        self.pdf = lambda x: self.rate * math.exp(-self.rate * x)


class JoMixtureSameFamily(JoDistribution):
    """
    Create a mixture distribution instance.
    """
    def __init__(self, probs, comp):
        self.probs = probs
        self.comp = comp
        self.pdf = lambda x: self.comp.log_prob(x).exp().view(1,-1)@self.probs.view(-1,1)  # simply linear combination.



def mining_aspects(reviews):
    """
    Reference: "Mining Opinion Features in Customer Reviews". Mingqing Hu and Bing Liu. AAAI-2004.
               "Mining and Summarizing Customer Reviews".     Mingqing Hu and Bing Liu. KDD'04.

    Usage:
    The input "reviews" should be a list of reviews.
        example:
            >>> reviews = ["Few films age as well as "Lilies of the Field". I saw it just recently \nfor the first tim...", "..."]
            >>> aspects = mining_aspects(reviews)
    """
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.tag import pos_tag
    from mlxtend.frequent_patterns import apriori
    from mlxtend.preprocessing import TransactionEncoder
    from itertools import chain
    sentences = [sent_tokenize(reviews[i].lower()) for i in range(len(reviews))]
    flatten = list(chain.from_iterable(sentences))
    tagged_words = []
    for sentence in flatten:
        words = word_tokenize(sentence)
        tagged_words.extend(pos_tag(words))

    # Extract nouns from tagged words
    explicit_features = [word for word, pos in tagged_words if pos.startswith('NN')]
    implicit_features = [word for word, pos in tagged_words if pos.startswith('JJ')]

    explicit_transactions = [[feature] for feature in explicit_features]
    te = TransactionEncoder()
    te_ary = te.fit(explicit_transactions).transform(explicit_transactions)
    df = pd.DataFrame(te_ary, columns=te.columns_)
    # Perform frequent itemset mining
    frequent_itemsets = apriori(df, min_support=0.01, use_colnames=True)
    frequent_nouns = frequent_itemsets[frequent_itemsets['itemsets'].apply(lambda x: len(x) == 1)]
    explicit_features = [next(iter(itemset)) for itemset in frequent_nouns.sort_values("support", ascending=False).itemsets]

    implicit_transactions = [[feature] for feature in implicit_features]
    te = TransactionEncoder()
    te_ary = te.fit(implicit_transactions).transform(implicit_transactions)
    df = pd.DataFrame(te_ary, columns=te.columns_)
    # Perform frequent itemset mining
    frequent_itemsets = apriori(df, min_support=0.01, use_colnames=True)
    frequent_nouns = frequent_itemsets[frequent_itemsets['itemsets'].apply(lambda x: len(x) == 1)]
    implicit_features = [next(iter(itemset)) for itemset in frequent_nouns.sort_values("support", ascending=False).itemsets]

    return (explicit_features, implicit_features)


def compute_ndcg_k(users_ground, users_ranked, k):
    """
    example:
        >>> users_ground = {0: [0, 10, 302, 365, 656, 665, 751], 2: [0,2,3,24,24,24]}
        >>> users_topk = {0:[0,1,2,124], 1:[2,3,235,25]}
    """
    ndcg_values = []

    for user in users_ground:
        ground_truth = users_ground[user]
        predicted_rankings = user_ranked[user][:k]

        relevance_scores = [1 if item in ground_truth else 0 for item in predicted_rankings]
        dcg = np.sum(relevance_scores / np.log2(np.arange(2, len(relevance_scores) + 2)))

        ideal_rankings = sorted(ground_truth, reverse=True)[:k]
        ideal_scores = [1] * len(ideal_rankings)
        idcg = np.sum(ideal_scores / np.log2(np.arange(2, len(ideal_scores) + 2)))
        ndcg = dcg / idcg
        ndcg_values.append(ndcg)

    return np.mean(ndcg_values)


def compute_recall_k(users_ground, users_ranked, k):
    recall_values = []
    for user in users_ground:
        ground_truth = users_ground[user]
        predicted_rankings = users_ranked[user][:k]
        relevant_items_count = len(set(ground_truth) & set(predicted_rankings))
        recall = relevant_items_count / len(ground_truth)
        recall_values.append(recall)
    return np.mean(recall_values)


def generate_random_samples(users_history, num_items, num_samples=100):
    """
    Example:
        >>> users_history = train_df.groupby("UserID")["ItemID"].apply(list)
        >>> generate_random_samples(users_histroy, ...)
    """
    random_samples = {}
    item_set = set(range(num_items))

    for user, items in users_history.items():
        historical_items = users_history[user]
        non_historical_items = item_set - set(historical_items)
        samples = random.sample(non_historical_items, num_samples)
        random_samples[user] = samples
    return pd.Series(random_samples)


def merge_samples_with_ground(random_samples, users_ground):
    """
    "random_samples" and "users_ground" is the same type
    """
    data = []
    for user, ground in users_ground.items():
        samples = random_samples.get(user)
        candidates = set(ground).union(set(samples))
        data.append([user, candidates, ground])
    df = pd.DataFrame(data, columns=["UserID", "candidates", "ground"])
    return df


def scrawl_papers(url, urlstyle, keywords):
    import requests
    from bs4 import BeautifulSoup
    import re
    """
    Example:
        >>> url =  "https://ir.webis.de/anthology/events/cikm_conference-2022/"
        >>> urlstyle = "anthology" (or, dblp)
        >>> keywords = ["explainable", "explaining", "explanation"]
    """
    print("Start crawling... please wait for a while...")
    response = requests.get(url)
    if response.status_code != 200:
        print("Failed to fetch the webpage.")
        exit()
    soup = BeautifulSoup(response.text, "html.parser")
    if urlstyle == "anthology":
        paper_titles = soup.find_all("a", class_="align-middle") 
    elif urlstyle == "dblp":
        paper_titles = soup.find_all("span", class_="title") 
    else:
        raise Exception("Sorry, current version only supports anthology and dblp source!")
    keywords = r"|".join(keywords)
    pattern = re.compile(keywords, re.IGNORECASE)
    matched_papers = []
    for title in paper_titles:
        if pattern.search(title.text):
            matched_papers.append(title.text.strip())
    print(f"{len(matched_papers)} papers found!")
    return matched_papers


def scrawl_papers_plus(url, urlstyle, keywords, acl_anthology):
    """
    This is a incremental version of func scrawl_papers, that this function finds also abstract with the title.
    Example:  
        >>> url = "https://ir.webis.de/anthology/events/cikm_conference-2022/"
        >>> urlstyle = "anthology"
        >>> keywords = ["explainable", "explaining", "explanation"]
        >>> acl_anthology = False
    """
    response = requests.get(url)
    if response.status_code != 200:
        print("Failed to fetch the webpage.")
    soup = BeautifulSoup(response.text, "html.parser")
    if urlstyle == "anthology":
        paper_titles = soup.find_all("a", class_="align-middle") 
    elif urlstyle == "dblp":
        paper_titles = soup.find_all("span", class_="title") 
    else:
        raise Exception("Sorry, current version only supports anthology and dblp source!")
    keywords = r"|".join(keywords)
    pattern = re.compile(keywords, re.IGNORECASE)
    matched_papers = []
    for title in paper_titles:
        if pattern.search(title.text):
            paper_title = title.text.strip()
            if acl_anthology: # ACL anthology 
                paper_url = "https://aclanthology.org/"+title.get("href")
                paper_response = requests.get(paper_url)
                paper_soup = BeautifulSoup(paper_response.text, "html.parser")
                abstract = paper_soup.find('div', class_='acl-abstract')
                if abstract: # if not none
                    paper_abstract = abstract.span.text.strip()
                else: 
                    paper_abstract = -1
                    
            elif urlstyle == "anthology": # abort
                paper_url = "https://ir.webis.de/"+title.get("href")
                paper_response = requests.get(paper_url)
                paper_soup = BeautifulSoup(paper_response.text, "html.parser") # find doi
                doi_url = paper_soup.find('meta', attrs={'property': 'og:url'})
                if doi_url: 
                    doi_url = doi_url["content"]
                    paper_response = requests.get(doi_url)
                    paper_soup = BeautifulSoup(paper_response.text, "html.parser")
                    abstract = paper_soup.find('div', class_='abstractSection abstractInFull')
                    if abstract: # if not none
                        paper_abstract = abstract.get_text(strip=True)
                    else: 
                        paper_abstract = -1

            else: # dblp
                doi_url = title.parent.parent.find_all("a")[1].get("href")
                if doi_url: 
                    # doi_url = doi_url["content"]
                    time.sleep(120)   # get around the API block problem.
                    paper_response = requests.get(doi_url)
                    paper_soup = BeautifulSoup(paper_response.text, "html.parser")
                    abstract = paper_soup.find('div', class_='abstractSection abstractInFull')
                    if abstract: # if not none
                        paper_abstract = abstract.get_text(strip=True)
                    else: 
                        paper_abstract = -1
                else:
                    paper_abstract = -1
            matched_papers.append((paper_title, paper_abstract))
    return matched_papers


def eigenvalues_eigenvectors_2x2(matrix):
    # Check if the matrix is 2x2
    if matrix.shape != (2, 2):
        raise ValueError("Matrix must be 2x2")

    a, b, c, d = matrix[0, 0], matrix[0, 1], matrix[1, 0], matrix[1, 1]

    # Compute the eigenvalues using the characteristic equation
    trace = a + d
    det = a * d - b * c
    discriminant = trace ** 2 - 4 * det

    if discriminant < 0:
        eigenvalues = np.array([trace / 2, np.sqrt(-discriminant) / 2])
    else:
        eigenvalues = np.array([trace / 2 + np.sqrt(discriminant) / 2,
                                trace / 2 - np.sqrt(discriminant) / 2])

    # Compute the eigenvectors
    eigenvectors = []
    for eigval in eigenvalues:
        v = np.array([1, (eigval - a) / b])
        v /= np.linalg.norm(v)
        eigenvectors.append(v)

    return eigenvalues, eigenvectors



def eigenvalues_eigenvectors_3x3(matrix):
    # Check if the matrix is 3x3
    if matrix.shape != (3, 3):
        raise ValueError("Matrix must be 3x3")

    # Compute the coefficients for the characteristic equation
    a = 1.0
    b = -np.trace(matrix)
    c = (np.trace(matrix) ** 2 - np.trace(np.matmul(matrix, matrix))) / 2.0
    d = -np.linalg.det(matrix)

    # Solve the cubic characteristic equation
    eigenvalues = np.roots([a, b, c, d])

    # Initialize a list to store eigenvectors
    eigenvectors = []

    # Find the eigenvectors for each eigenvalue
    for eigenvalue in eigenvalues:
        # Solve (A - λI) * v = 0 for eigenvectors
        eigenvector = np.linalg.solve(matrix - eigenvalue * np.identity(3), np.zeros(3))
        eigenvectors.append(eigenvector)

    return eigenvalues, eigenvectors


def is_diagonalizable(matrix):
    # Calculate eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eig(matrix)
    
    # Check if eigenvalues have equal algebraic and geometric multiplicities
    for eigenvalue in eigenvalues:
        algebraic_multiplicity = np.sum(eigenvalues == eigenvalue)
        geometric_multiplicity = np.sum(np.isclose(eigenvalues, eigenvalue))
        if algebraic_multiplicity != geometric_multiplicity:
            return False
    return True


def is_positive_definite(A):
    """
    Check if matrix A is positive definite.
    """
    # Ensure the matrix is symmetric
    if not np.allclose(A, A.T):
        return False
    
    # Check if all eigenvalues are positive
    eigenvalues = np.linalg.eigvalsh(A)
    return np.all(eigenvalues > 0)


def compute_svd(A):
    """
    U, S, VT = np.linalg.svd(A)
    """
    B = A@A.T
    U, S_2 = np.linalg.eig(B)


    C = A.T@A
    V, S_2 = np.linalg.eig(C)

    S = np.sqrt(S_2)
    return U, S, V


def PCAfrom_scratch(matrix, ncomponents):
    """
    feature_matrix should be in m x n shape, where m is the number of data, n is the number of features.
    for example, 
        >>> feature_matrix = np.random.randn(100, 5)
        >>> compressed_features, new_feature_vectors = PCAfrom_scratch(feature_matrix)
    """
    # centering matrix
    mean = np.mean(matrix, axis=0)
    std = np.std(matrix, axis=0)
    standard_ = (matrix-mean)/std


    # compute covariance matrix
    S = np.cov(standard_, rowvar=False)

    # perform svd decomposition or eigenvector decomposition. They are equivelent due to the symmetrics of covariance matrix. 
    U, Lambda, Vt = np.linalg.svd(S)
    transformed_data = standard_ @ U[:,:ncomponents]
    return transformed_data, U, Lambda


def LDAfrom_scratch():
    pass


def KL_divergence_fromscratch(dist1, dist2):
    """
    dist1 and dist2 are discrete probability distributions.
    Example: 
        >>> dist1 = [0.5, 0.5]
        >>> dist2 = [0.4, 0.6]
        >>> KL_divergence_fromscratch(dist1, dist2)
    """
    length = len(dist1)
    kl_divergence = 0
    for i in range(length):
        kl_divergence += dist1[i]*np.log(dist1[i]/dist2[i])

    return kl_divergence


def JS_distance_fromscratch():
    """
    dist1 and dist2 are discrete probability distributions.
    Example: 
        >>> dist1 = [0.5, 0.5]
        >>> dist2 = [0.4, 0.6]
        >>> JS_distance_fromscratch(dist1, dist2)
    """
    length = len(dist1)
    distm = [(dist1[i]+dist2[i])/2 for i in range(length)]
    kl1 = 0
    length = len(dist1)
    for i in range(length):
        kl1 += dist1[i]*np.log2(dist1[i]/distm[i])


    kl2 = 0
    for i in range(length):
        kl2 += dist2[i]*np.log2(dist2[i]/distm[i])

    js_distance = (kl1+kl2)/2
    return js_distance


def Wasserstein_distance_fromscratch():
    pass


def check_lipschitz_condition(f, x1, x2, L):
    """
    lipschitz_condition means that, the function does not change too rapidly, beucase we can always find a positive number that constraints the slope.
    """
    return abs(f(x1) - f(x2)) <= L * abs(x1 - x2)


def analyze_log(log.txt):
    # Initialize variables to track the best hyperparameters and performance

    log_file_path = "log.txt"  # Replace with the actual path to your log file
    performance_values = []
    with open(log_file_path, "r") as log_file:
        for line in log_file:
            if "Evaluation performance:" in line:
                # Extract the performance value (assuming it's a float)
                try:
                    performance = float(line.split("Evaluation performance:")[1].strip())
                    performance_values.append(performance)
                except ValueError:
                    # Handle cases where the value couldn't be converted to float
                    pass


    # Define the learning rates and weight decay values
    learning_rates = ["0.00005", "0.0001", "0.0005", "0.001", "0.005", "0.01", "0.05", "0.1", "0.5"]
    weight_decay = ["0.00005", "0.0001", "0.0005", "0.001", "0.005", "0.01"]


    # Initialize dictionaries to store performance averages for each hyperparameter
    lr_averages = {lr: [] for lr in learning_rates}
    wd_averages = {wd: [] for wd in weight_decay}

    # Organize performance results into dictionaries
    for i, lr in enumerate(learning_rates):
        for j, wd in enumerate(weight_decay):
            lr_averages[lr].append(performance_values[i * len(weight_decay) + j])
            wd_averages[wd].append(performance_values[i * len(weight_decay) + j])

    # Compute the average performance for each learning rate and weight decay
    lr_average_performance = [np.mean(lr_averages[lr]) for lr in learning_rates]
    wd_average_performance = [np.mean(wd_averages[wd]) for wd in weight_decay]

    # Create line charts for average performance by learning rate and weight decay
    plt.figure(figsize=(12, 6))

    # Line chart for learning rates
    plt.subplot(1, 2, 1)
    plt.plot(learning_rates, lr_average_performance, marker='o')
    plt.xlabel('Learning Rates')
    plt.ylabel('Average Performance')
    plt.title('Average Performance vs. Learning Rates')
    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability

    # Line chart for weight decay
    plt.subplot(1, 2, 2)
    plt.plot(weight_decay, wd_average_performance, marker='o')
    plt.xlabel('Weight Decay')
    plt.ylabel('Average Performance')
    plt.title('Average Performance vs. Weight Decay')
    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability

    # Adjust spacing between subplots
    plt.tight_layout()

    # Show the plots
    plt.show()



    # output best paremter set
    best_performance = float('inf')  # Initialize with a high value
    best_lr = None
    best_wd = None
    for i, lr in enumerate(learning_rates):
        for j, wd in enumerate(weight_decay):
            performance = performance_values[i * len(weight_decay) + j]
            if performance < best_performance:
                best_performance = performance
                best_lr = lr
                best_wd = wd

    # Print the best hyperparameters and performance
    print("Best Hyperparameters:")
    print("Learning Rate:", best_lr)
    print("Weight Decay:", best_wd)
    print("Best Performance:", best_performance)
